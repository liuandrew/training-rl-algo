{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import gym_nav\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from a2c_ppo_acktr import algo, utils\n",
    "from a2c_ppo_acktr.algo import gail\n",
    "from a2c_ppo_acktr.arguments import get_args\n",
    "from a2c_ppo_acktr.envs import make_vec_envs\n",
    "from a2c_ppo_acktr.model import Policy\n",
    "from a2c_ppo_acktr.storage import RolloutStorage\n",
    "from evaluation import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andy\\miniconda3\\lib\\site-packages\\gym\\wrappers\\record_video.py:42: UserWarning: \u001b[33mWARN: Overwriting existing videos at C:\\Users\\Andy\\Desktop\\Work\\github\\training-rl-algo\\video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  f\"Overwriting existing videos at {self.video_folder} folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 80, num timesteps 405, FPS 70 \n",
      " Last 2 training episodes: mean/median reward 0.2/0.2, min/max reward 0.1/0.2\n",
      "\n",
      "Updates 90, num timesteps 455, FPS 70 \n",
      " Last 2 training episodes: mean/median reward 0.2/0.2, min/max reward 0.1/0.2\n",
      "\n",
      "Updates 100, num timesteps 505, FPS 66 \n",
      " Last 2 training episodes: mean/median reward 0.2/0.2, min/max reward 0.1/0.2\n",
      "\n",
      "Updates 110, num timesteps 555, FPS 66 \n",
      " Last 2 training episodes: mean/median reward 0.2/0.2, min/max reward 0.1/0.2\n",
      "\n",
      "Updates 120, num timesteps 605, FPS 65 \n",
      " Last 3 training episodes: mean/median reward 0.2/0.1, min/max reward 0.1/0.2\n",
      "\n",
      "Updates 130, num timesteps 655, FPS 66 \n",
      " Last 3 training episodes: mean/median reward 0.2/0.1, min/max reward 0.1/0.2\n",
      "\n",
      "Updates 140, num timesteps 705, FPS 64 \n",
      " Last 3 training episodes: mean/median reward 0.2/0.1, min/max reward 0.1/0.2\n",
      "\n",
      "Updates 150, num timesteps 755, FPS 65 \n",
      " Last 3 training episodes: mean/median reward 0.2/0.1, min/max reward 0.1/0.2\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-91fceb628d54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0malg\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'ppo'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mvalue_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist_entropy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapprox_kl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclipfracs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Work\\github\\training-rl-algo\\a2c_ppo_acktr\\algo\\ppo.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, rollouts)\u001b[0m\n\u001b[0;32m     87\u001b[0m                     \u001b[0mvalue_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreturn_batch\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m                 (value_loss * self.value_loss_coef + action_loss -\n\u001b[0;32m     91\u001b[0m                  dist_entropy * self.entropy_coef).backward()\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_zero_grad_profile_name\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hook_for_profile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zero_grad_profile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\autograd\\profiler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[1;31m# Stores underlying RecordFunction as a tensor. TODO: move to custom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;31m# class (https://github.com/pytorch/pytorch/issues/35026).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 613\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_name = 'Gridworld-v0'\n",
    "log_dir = '/tmp/gym'\n",
    "device = torch.device(\"cpu\")\n",
    "alg = 'ppo'\n",
    "log_interval = 10\n",
    "\n",
    "value_loss_coef = 0.5\n",
    "entropy_coef = 0.01\n",
    "gamma = 0.99\n",
    "lr = 7e-4\n",
    "eps = 1e-5\n",
    "alpha = 0.99\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "clip_param = 0.2\n",
    "ppo_epoch = 4\n",
    "num_mini_batch = 1\n",
    "\n",
    "num_env_steps = 10000\n",
    "num_steps = 5\n",
    "num_processes = 1\n",
    "\n",
    "use_gae = False\n",
    "gae_lambda = 0.95\n",
    "use_proper_time_limits = False\n",
    "\n",
    "env = gym.make(env_name)\n",
    "envs = make_vec_envs(env_name, 0, 1, gamma, log_dir, device, False, capture_video=1, \n",
    "                     env_kwargs={'reward_shaping': 2})\n",
    "\n",
    "actor_critic = Policy(\n",
    "    envs.observation_space.shape,\n",
    "    envs.action_space,\n",
    "    base_kwargs={'recurrent': True})\n",
    "actor_critic.to(device)\n",
    "\n",
    "if alg == 'a2c':\n",
    "        agent = algo.A2C_ACKTR(\n",
    "            actor_critic,\n",
    "            value_loss_coef,\n",
    "            entropy_coef,\n",
    "            lr=lr,\n",
    "            eps=eps,\n",
    "            alpha=alpha,\n",
    "            max_grad_norm=max_grad_norm)\n",
    "elif alg == 'ppo':\n",
    "    agent = algo.PPO(\n",
    "        actor_critic,\n",
    "        clip_param,\n",
    "        ppo_epoch,\n",
    "        num_mini_batch,\n",
    "        value_loss_coef,\n",
    "        entropy_coef,\n",
    "        lr=lr,\n",
    "        eps=eps,\n",
    "        max_grad_norm=max_grad_norm)\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "rollouts = RolloutStorage(num_steps, num_processes,\n",
    "                          envs.observation_space.shape, envs.action_space,\n",
    "                          actor_critic.recurrent_hidden_state_size)\n",
    "\n",
    "obs = envs.reset()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "episode_rewards = deque(maxlen=10)\n",
    "\n",
    "start = time.time()\n",
    "num_updates = int(\n",
    "    num_env_steps) // num_steps // num_processes\n",
    "for j in range(num_updates):\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        #Andy: add global step\n",
    "        global_step += 1 * num_processes\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                rollouts.obs[step], rollouts.recurrent_hidden_states[step],\n",
    "                rollouts.masks[step])\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, infos = envs.step(action)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor(\n",
    "            [[0.0] if done_ else [1.0] for done_ in done])\n",
    "        bad_masks = torch.FloatTensor(\n",
    "            [[0.0] if 'bad_transition' in info.keys() else [1.0]\n",
    "             for info in infos])\n",
    "        rollouts.insert(obs, recurrent_hidden_states, action,\n",
    "                        action_log_prob, value, reward, masks, bad_masks)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = actor_critic.get_value(\n",
    "            rollouts.obs[-1], rollouts.recurrent_hidden_states[-1],\n",
    "            rollouts.masks[-1]).detach()\n",
    "        \n",
    "    rollouts.compute_returns(next_value, use_gae, gamma,\n",
    "                             gae_lambda, use_proper_time_limits)\n",
    "\n",
    "    if alg == 'ppo':\n",
    "        value_loss, action_loss, dist_entropy, approx_kl, clipfracs = \\\n",
    "        agent.update(rollouts)\n",
    "\n",
    "    else:\n",
    "        value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "\n",
    "    rollouts.after_update()\n",
    "\n",
    "    if j % log_interval == 0 and len(episode_rewards) > 1:\n",
    "        total_num_steps = (j + 1) * num_processes * num_steps\n",
    "        end = time.time()\n",
    "        print(\n",
    "            \"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"\n",
    "            .format(j, total_num_steps,\n",
    "                    int(total_num_steps / (end - start)),\n",
    "                    len(episode_rewards), np.mean(episode_rewards),\n",
    "                    np.median(episode_rewards), np.min(episode_rewards),\n",
    "                    np.max(episode_rewards), dist_entropy, value_loss,\n",
    "                    action_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Gridworld-v0', reward_shaping=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 6., 1., 1.]), 1.01, True, {})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
