{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab78974-d11d-4f06-a7b5-b1ca6a3fa6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "%run model_evaluation\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import proplot as pplt\n",
    "import umap\n",
    "\n",
    "# model, obs_rms, kwargs = load_model_and_env('nav_auxiliary_tasks/nav_aux_wall_1', 0)\n",
    "# env = gym.make('NavEnv-v0', **kwargs)\n",
    "\n",
    "save = 'plots/representation_heatmaps/'\n",
    "\n",
    "%run representation_analysis\n",
    "%run model_evaluation\n",
    "\n",
    "\n",
    "def gaussian_smooth(pos, y, extent=(5, 295), num_grid=30, sigma=10,\n",
    "                    ret_hasval=False):\n",
    "    # a = stacked['shared_activations'][0, :, 0].numpy()\n",
    "    y = np.array(y)\n",
    "    \n",
    "    grid = np.linspace(extent[0], extent[1], num_grid)\n",
    "    xs, ys = np.meshgrid(grid, grid)\n",
    "    ys = ys[::-1]\n",
    "    smoothed = np.zeros(xs.shape)\n",
    "    hasval = np.zeros(xs.shape)\n",
    "    for i in range(num_grid):\n",
    "        for j in range(num_grid):\n",
    "            p = np.array([xs[i, j], ys[i, j]])\n",
    "            dists = np.sqrt(np.sum((pos - p)**2, axis=1))\n",
    "            g = np.exp(-dists**2 / (2*sigma**2))\n",
    "            \n",
    "            if len(g[g > 0.1]) < 1:\n",
    "                val = 0\n",
    "            else:\n",
    "                val = np.sum(y[g > 0.1] * g[g > 0.1]) / np.sum(g[g > 0.1])\n",
    "                hasval[i, j] = 1\n",
    "\n",
    "            smoothed[i, j] = val\n",
    "    if ret_hasval:\n",
    "        return smoothed, hasval\n",
    "    else:\n",
    "        return smoothed\n",
    "\n",
    "\n",
    "def clean_eps(eps, prune_first=5, activations_key='shared_activations',\n",
    "             activations_layer=0, clip=False,\n",
    "             save_inview=True, save_seen=True):\n",
    "    '''Clean up an eps data dictionary collected from evalu for heatmapping'''\n",
    "    dones = eps['dones'].copy()\n",
    "    pos = np.vstack(eps['data']['pos'])\n",
    "    stacked = stack_activations(eps['activations'])\n",
    "    angles = eps['data']['angle']\n",
    "    acts = eps['actions']\n",
    "    \n",
    "    activ = stacked[activations_key][activations_layer, :, :].numpy()\n",
    "    pinview = np.array(eps['data']['poster_in_view'])\n",
    "    pseen = np.array(eps['data']['poster_seen'])\n",
    "    \n",
    "    ep_activ = split_by_ep(activ, dones)\n",
    "    ep_pos = split_by_ep(pos, dones)\n",
    "    ep_pinview = split_by_ep(pinview, dones)\n",
    "    ep_angle = split_by_ep(angles, dones)\n",
    "    ep_pseen = split_by_ep(pseen, dones)\n",
    "    ep_acts = split_by_ep(acts, dones)\n",
    "    \n",
    "    if prune_first and prune_first > 0:\n",
    "        prune_first = 5\n",
    "        pruned_ep_activ = [a[prune_first:] for a in ep_activ]\n",
    "        pruned_activ = np.vstack(pruned_ep_activ)\n",
    "        pruned_ep_pos = [p[prune_first:] for p in ep_pos]\n",
    "        pruned_pos = np.vstack(pruned_ep_pos)\n",
    "        pruned_ep_pinview = [p[prune_first:] for p in ep_pinview]\n",
    "        pruned_pinview = np.concatenate(pruned_ep_pinview)\n",
    "        pruned_ep_angles = [p[prune_first:] for p in ep_angle]\n",
    "        pruned_angles = np.concatenate(pruned_ep_angles)\n",
    "        pruned_ep_pseen = [p[prune_first:] for p in ep_pseen]\n",
    "        pruned_pseen = np.concatenate(pruned_ep_pseen)\n",
    "        pruned_ep_acts = [p[prune_first:] for p in ep_acts]\n",
    "        pruned_acts = np.concatenate(pruned_ep_acts)\n",
    "        \n",
    "        pos = pruned_pos\n",
    "        activ = pruned_activ\n",
    "        pinview = pruned_pinview\n",
    "        angles = pruned_angles\n",
    "        pseen = pruned_pseen\n",
    "        acts = pruned_acts\n",
    "    \n",
    "    if clip:\n",
    "        activ = np.clip(activ, 0, 1)\n",
    "    \n",
    "    result_dict = {\n",
    "        'pos': pos,\n",
    "        'activ': activ,\n",
    "        'pinview': pinview,\n",
    "        'pseen': pseen,\n",
    "        'angles': angles,\n",
    "        'dones': dones,\n",
    "        'actions': acts\n",
    "    }\n",
    "    \n",
    "    if save_inview:\n",
    "        result_dict.update({\n",
    "            'pos_inview': pos[pinview],\n",
    "            'pos_notinview': pos[~pinview],\n",
    "            'activ_inview': activ[pinview],\n",
    "            'activ_notinview': activ[~pinview],\n",
    "            'angles_inview': angles[pinview],\n",
    "            'angles_notinview': angles[~pinview],\n",
    "        })\n",
    "    if save_seen:\n",
    "        result_dict.update({'pos_seen': pos[pseen],\n",
    "        'pos_notseen': pos[~pseen],\n",
    "        'activ_seen': activ[pseen],\n",
    "        'activ_notseen': activ[~pseen],\n",
    "        'angles_seen': angles[pseen],\n",
    "        'angles_notseen': angles[~pseen],\n",
    "        })\n",
    "    \n",
    "    return result_dict\n",
    "    \n",
    "    \n",
    "def stack_all_ep(all_ep):\n",
    "    '''\n",
    "    When making a list of results from multiple evalu calls,\n",
    "    this function can be called to put the relevant data into a single dict to be\n",
    "    passed to clean_eps for processing\n",
    "    '''\n",
    "    dones = np.concatenate([ep['dones'] for ep in all_ep])\n",
    "    pos = np.vstack([ep['data']['pos'] for ep in all_ep])\n",
    "    angles = np.concatenate([ep['data']['angle'] for ep in all_ep])\n",
    "    pseen = np.concatenate([ep['data']['poster_seen'] for ep in all_ep])\n",
    "    pinview = np.concatenate([ep['data']['poster_in_view'] for ep in all_ep])\n",
    "    actions = np.vstack([np.vstack(ep['actions']) for ep in all_ep]).squeeze()\n",
    "    activations = []\n",
    "    for ep in all_ep:\n",
    "        activations += ep['activations']\n",
    "\n",
    "    eps = {\n",
    "        'dones': dones,\n",
    "        'activations': activations,\n",
    "        'actions': actions,\n",
    "        'data': {\n",
    "            'pos': pos,\n",
    "            'angle': angles,\n",
    "            'poster_seen': pseen,\n",
    "            'poster_in_view': pinview\n",
    "        }\n",
    "    }\n",
    "    return eps\n",
    "    \n",
    "\n",
    "    \n",
    "def split_by_angle(target, angles):\n",
    "    splits = {\n",
    "        0: [-np.pi/4, np.pi/4],\n",
    "        1: [np.pi/4, 3*np.pi/4],\n",
    "        3: [-3*np.pi/4, -np.pi/4],\n",
    "        2: None #this will use else statement otherwise bounds are annoying\n",
    "    }\n",
    "    all_trues = np.zeros(angles.shape) == 1\n",
    "    result = {}\n",
    "    \n",
    "    for s in [0, 1, 3]:\n",
    "        split = splits[s]\n",
    "        split_idxs = (split[0] <= angles) & (angles <= split[1])\n",
    "        all_trues = all_trues | split_idxs\n",
    "        \n",
    "        result[s] = target[split_idxs]\n",
    "    #finally, the ones that didn't fit into any of the other quadrants\n",
    "    result[2] = target[~all_trues]\n",
    "    \n",
    "    return result\n",
    "    \n",
    "        \n",
    "    \n",
    "def compute_directness(all_ep=None, ep=None, pos=None):\n",
    "    '''\n",
    "    Compute the directness of paths taken either from an all_ep (split up\n",
    "    eps generated from appending evalu() calls) or from a single ep\n",
    "    '''\n",
    "    goal_loc = np.array([250, 70])\n",
    "    if all_ep is None and ep is None and pos is None:\n",
    "        raise Exception('No proper parameters given')\n",
    "\n",
    "    if all_ep is not None:\n",
    "        directnesses = []\n",
    "        for i in range(len(all_ep)):\n",
    "            p = np.vstack(all_ep[i]['data']['pos'])\n",
    "            d = p - goal_loc\n",
    "            d = np.sqrt(np.sum(d**2, axis=1))\n",
    "            dist_changes = np.diff(d)\n",
    "            directness = np.sum(dist_changes[:-1] < 0) / np.sum(dist_changes[:-1] != 0)\n",
    "            directnesses.append(directness)\n",
    "        return np.array(directnesses)\n",
    "    else:\n",
    "        if ep is not None:\n",
    "            p = np.vstack(ep['data']['pos'])\n",
    "        elif pos is not None:\n",
    "            p = pos\n",
    "        d = p - goal_loc\n",
    "        d = np.sqrt(np.sum(d**2, axis=1))\n",
    "        dist_changes = np.diff(d)\n",
    "        directness = np.sum(dist_changes[:-1] < 0) / np.sum(dist_changes[:-1] != 0)\n",
    "        return directness\n",
    "    \n",
    "        \n",
    "            \n",
    "            \n",
    "def filter_all_ep_directness(all_ep, bound=0.9):\n",
    "    d = compute_directness(all_ep)\n",
    "    idxs = d > 0.9\n",
    "    d_ep = [ep for i, ep in enumerate(all_ep) if idxs[i]]\n",
    "    return d_ep\n",
    "\n",
    "\n",
    "\n",
    "def load_heatmaps(file='data/pdistal_rim_heatmap/rim_heatmaps'):\n",
    "    all_heatmaps = pickle.load(open(file, 'rb'))\n",
    "\n",
    "    heatmaps = []\n",
    "    heatmap_idx_to_model = []\n",
    "    heatmap_model_to_idxs = {}\n",
    "    widths = [4, 8, 16, 32, 64]\n",
    "    trials = 3\n",
    "\n",
    "    current_idx = 0\n",
    "    for width in widths:\n",
    "        heatmap_model_to_idxs[width] = []\n",
    "        for trial in range(trials):\n",
    "            heatmaps.append(all_heatmaps[width][trial])\n",
    "\n",
    "            #create indexers to map back and forth between heatmap idxs and models\n",
    "            for i in range(width):\n",
    "                heatmap_idx_to_model.append([width, trial, i])\n",
    "            heatmap_model_to_idxs[width].append([current_idx, current_idx+width])\n",
    "            current_idx = current_idx + width\n",
    "\n",
    "    heatmaps = np.clip(np.vstack(heatmaps).reshape(372, 900), 0, 1)\n",
    "    return heatmaps, heatmap_idx_to_model, heatmap_model_to_idxs\n",
    "\n",
    "\n",
    "def count_labels(clabels, ignore_cluster=None, remove_zeros=False):\n",
    "    #Convert a list of cluster labels into ratios\n",
    "    cluster_counts = np.zeros(num_clusters)\n",
    "    for i in range(num_clusters):\n",
    "        cluster_counts[i] = np.sum(clabels == i)\n",
    "        \n",
    "    if ignore_cluster is not None:\n",
    "        if type(ignore_cluster) == list:\n",
    "            for c in ignore_cluster:\n",
    "                cluster_counts[c] = 0\n",
    "        elif type(ignore_cluster) == int:\n",
    "            cluster_counts[ignore_cluster] = 0\n",
    "    \n",
    "    cluster_ratios = cluster_counts / np.sum(cluster_counts)\n",
    "    \n",
    "    if remove_zeros:\n",
    "        cluster_ratios = cluster_ratios[cluster_ratios != 0]\n",
    "        cluster_counts = cluster_counts[cluster_counts != 0]\n",
    "    return cluster_counts, cluster_ratios\n",
    "\n",
    "\n",
    "\n",
    "def pred_kmeans(heatmaps, kmeans):\n",
    "    '''\n",
    "    Given a list of heatmaps, perform necessary reshaping and predict cluster with kmeans\n",
    "    '''\n",
    "    hms = np.vstack([hm.reshape(1, -1) for hm in heatmaps])\n",
    "    labels = kmeans.predict(hms)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c8dee-805e-4b15-9480-18cd7b9d70b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preliminary Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb9a68b-6220-4594-90ca-35cffc1d0f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(ncols=4)\n",
    "for i in range(3):\n",
    "    ep = pickle.load(open(f'data/pdistal_rim_heatmap/width64_t{i}', 'rb'))\n",
    "    \n",
    "    ax[i].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    ax[3].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f35efb-9d6c-48d0-b277-bb21416ee7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(ncols=4)\n",
    "for i in range(3):\n",
    "    ep = pickle.load(open(f'data/pdistal_rim_heatmap/width64_filt_t{i}', 'rb'))\n",
    "    \n",
    "    ax[i].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    ax[3].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de6b354-b42b-41e7-b4f5-f54c68edba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(ncols=4)\n",
    "for i in range(3):\n",
    "    ep = pickle.load(open(f'data/pdistal_rim_heatmap/width64_t{i}', 'rb'))\n",
    "    \n",
    "    ax[i].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    ax[3].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73c694-541d-4bb0-ac8d-4cb5c73e602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(ncols=4)\n",
    "for i in range(3):\n",
    "    ep = pickle.load(open(f'data/pdistal_rim_heatmap/width32_t{i}', 'rb'))\n",
    "    \n",
    "    ax[i].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    ax[3].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0becb613-76d8-4cbc-baac-ba28dcced1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(ncols=4)\n",
    "for i in range(3):\n",
    "    ep = pickle.load(open(f'data/pdistal_rim_heatmap/width16_t{i}', 'rb'))\n",
    "    \n",
    "    ax[i].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    ax[3].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24453937-29b2-4be5-9edf-c4b022015c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(ncols=4)\n",
    "for i in range(3):\n",
    "    ep = pickle.load(open(f'data/pdistal_rim_heatmap/width8_t{i}', 'rb'))\n",
    "    \n",
    "    ax[i].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    ax[3].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a668b-bc9f-4507-9f39-acda1dfa7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(ncols=4)\n",
    "for i in range(3):\n",
    "    ep = pickle.load(open(f'data/pdistal_rim_heatmap/width4_t{i}', 'rb'))\n",
    "    \n",
    "    ax[i].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    ax[3].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689281bb-ccd4-4cf5-89ea-b7fabadf5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(ncols=4)\n",
    "for i in range(3):\n",
    "    ep = pickle.load(open(f'data/pdistal_rim_heatmap/width3_t{i}', 'rb'))\n",
    "    \n",
    "    ax[i].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    ax[3].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462575a8-e5e2-4dd3-a196-b7f8e4a39406",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(ncols=4)\n",
    "for i in range(3):\n",
    "    ep = pickle.load(open(f'data/pdistal_rim_heatmap/width2_t{i}', 'rb'))\n",
    "    \n",
    "    ax[i].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    ax[3].scatter(ep['pos'].T[0], ep['pos'].T[1], alpha=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c1793-c839-43a3-ad8a-f0da4aacec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = pickle.load(open(f'data/pdistal_rim_heatmap/width32_t2', 'rb'))\n",
    "fig, ax = pplt.subplots(nrows=4, ncols=8, wspace=0, hspace=0)\n",
    "for i in range(32):\n",
    "    heatmap = np.clip(gaussian_smooth(ep['pos'], ep['activ'][:, i]), 0, 1)\n",
    "    ax[i].imshow(heatmap, extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c868a6-0be8-4d25-a2f2-e429c8791ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name1 = 'nav_poster_netstructure/nav_pdistal_width64batch200'\n",
    "model1, obs_rms1, kwargs = load_model_and_env(model_name1, 2)\n",
    "model_name2 = 'nav_poster_netstructure/nav_pdistal_width32batch200'\n",
    "model2, obs_rms2, kwargs = load_model_and_env(model_name2, 2)\n",
    "\n",
    "#Starting around rim - First generate start points and angles\n",
    "WINDOW_SIZE = (300, 300)\n",
    "step_size = 10.\n",
    "xs = np.arange(0+step_size, WINDOW_SIZE[0], step_size)\n",
    "ys = np.arange(0+step_size, WINDOW_SIZE[1], step_size)\n",
    "# thetas = np.linspace(0, 2*np.pi, 12, endpoint=False)\n",
    "start_points = []\n",
    "start_angles = []\n",
    "for x in xs:\n",
    "    for y in [5., 295.]:\n",
    "        point = np.array([x, y])\n",
    "        angle = np.arctan2(150 - y, 150 - x)\n",
    "        start_points.append(point)\n",
    "        start_angles.append(angle)\n",
    "for y in ys:\n",
    "    for x in [5, 295]:\n",
    "        point = np.array([x, y])\n",
    "        angle = np.arctan2(150 - y, 150 - x)\n",
    "        start_points.append(point)\n",
    "        start_angles.append(angle)\n",
    "        \n",
    "start_points = np.vstack(start_points)\n",
    "\n",
    "all_ep1 = []\n",
    "for i in range(len(start_points)):\n",
    "    kw = kwargs.copy()\n",
    "    kw['fixed_reset'] = [start_points[i].copy(), start_angles[i].copy()]\n",
    "    ep = forced_action_evaluate(model1, obs_rms1, seed=0, num_episodes=1, \n",
    "                                env_kwargs=kw, data_callback=poster_data_callback,\n",
    "                                with_activations=True)\n",
    "    all_ep1.append(ep)\n",
    "saved_actions = [ep['actions'] for ep in all_ep1]\n",
    "\n",
    "\n",
    "#Force CW actions to CCW model\n",
    "all_ep2 = []\n",
    "for i in range(len(start_points)):\n",
    "    copied_actions = lambda step: saved_actions[i][step]\n",
    "    kw = kwargs.copy()\n",
    "    kw['fixed_reset'] = [start_points[i].copy(), start_angles[i].copy()]\n",
    "    ep = forced_action_evaluate(model2, obs_rms2, seed=0, num_episodes=1, \n",
    "                                env_kwargs=kw, data_callback=poster_data_callback,\n",
    "                                with_activations=True, forced_actions=copied_actions)\n",
    "    all_ep2.append(ep)\n",
    "# saved_actions = [ep['actions'] for ep in all_ep2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310a4b1-4f46-4cbc-8c71-0f0ff0c4b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = clean_eps(stack_all_ep(all_ep2), prune_first=0)\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=4, ncols=8, wspace=0, hspace=0)\n",
    "for i in range(32):\n",
    "    heatmap = np.clip(gaussian_smooth(ep['pos'], ep['activ'][:, i]), 0, 1)\n",
    "    ax[i].imshow(heatmap, extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff990634-f2e8-4794-94d0-42ad36a6801b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Collection (Saved files in each section in parenthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9346a595-65ae-4e9b-92ca-3996c8ca39b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_ep = [[],[],[]]\n",
    "width = 64\n",
    "\n",
    "#Starting around rim - First generate start points and angles\n",
    "WINDOW_SIZE = (300, 300)\n",
    "step_size = 10.\n",
    "xs = np.arange(0+step_size, WINDOW_SIZE[0], step_size)\n",
    "ys = np.arange(0+step_size, WINDOW_SIZE[1], step_size)\n",
    "# thetas = np.linspace(0, 2*np.pi, 12, endpoint=False)\n",
    "start_points = []\n",
    "start_angles = []\n",
    "for x in xs:\n",
    "    for y in [5., 295.]:\n",
    "        point = np.array([x, y])\n",
    "        angle = np.arctan2(150 - y, 150 - x)\n",
    "        start_points.append(point)\n",
    "        start_angles.append(angle)\n",
    "for y in ys:\n",
    "    for x in [5, 295]:\n",
    "        point = np.array([x, y])\n",
    "        angle = np.arctan2(150 - y, 150 - x)\n",
    "        start_points.append(point)\n",
    "        start_angles.append(angle)\n",
    "        \n",
    "for trial in range(3):\n",
    "    model_name = f'nav_poster_netstructure/nav_pdistal_width64batch200'\n",
    "    model, obs_rms, kwargs = load_model_and_env(model_name, trial)\n",
    "\n",
    "    all_ep = []\n",
    "    for i in range(len(start_points)):\n",
    "        kw = kwargs.copy()\n",
    "        kw['fixed_reset'] = [start_points[i].copy(), start_angles[i].copy()]\n",
    "        ep = forced_action_evaluate(model, obs_rms, seed=0, num_episodes=1, \n",
    "                                    env_kwargs=kw, data_callback=poster_data_callback,\n",
    "                                    with_activations=True)\n",
    "        all_ep.append(ep)\n",
    "\n",
    "    all_ep_f = filter_all_ep_directness(all_ep)\n",
    "    eps_f = clean_eps(stack_all_ep(all_ep_f), prune_first=0, save_inview=False, save_seen=False)\n",
    "    eps = clean_eps(stack_all_ep(all_ep), prune_first=0, save_inview=False, save_seen=False)\n",
    "    \n",
    "    saved_actions = [ep['actions'] for ep in all_ep]\n",
    "    saved_actions_f = [ep['actions'] for ep in all_ep_f]\n",
    "    \n",
    "    pickle.dump(saved_actions, open(f'data/pdistal_rim_heatmap/width{width}_t{trial}_acts', 'wb'))\n",
    "    pickle.dump(saved_actions_f, open(f'data/pdistal_rim_heatmap/width{width}_filt_t{trial}_acts', 'wb'))\n",
    "    \n",
    "    pickle.dump(eps, open(f'data/pdistal_rim_heatmap/width{width}_t{trial}', 'wb'))\n",
    "    pickle.dump(eps_f, open(f'data/pdistal_rim_heatmap/width{width}_filt_t{trial}', 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e76fa-cf9c-4193-9c2f-1efac8e0c6d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Constructing some consistent trajectories to use (width64_comb)\n",
    "\n",
    "Here, we generate conserved trajectories to test all our agents on. The trajectories consist of the most direct trajectories with the good overall spatial coverage so we can accurately test activations without hallucinations across the entire area\n",
    "\n",
    "**Final paths are saved in data/pdistal_rim_heatmap/width64_comb** which contains (combined_actions, start_points, start_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5bcdff-0676-495f-8744-d91802c75301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t{trial}', 'rb'))\n",
    "saved_actions = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t{trial}_acts', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd0c763-6ae4-4501-9803-bfa52dbc0117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Figuring out where the trajectories differ\n",
    "\n",
    "start = 24\n",
    "\n",
    "fig, ax = pplt.subplots(ncols=3)\n",
    "ax.format(xlim=[0, 300], ylim=[0, 300])\n",
    "\n",
    "for trial in range(3):\n",
    "    eps = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t{trial}', 'rb'))\n",
    "    saved_actions = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t{trial}_acts', 'rb'))\n",
    "\n",
    "    pos = eps['pos']\n",
    "    dones = eps['dones']\n",
    "    all_pos = np.vstack(split_by_ep(pos, dones)[start:end])\n",
    "    # plt.scatter(eps['pos'][start:end].T[0], eps['pos'][start:end].T[1])\n",
    "    ax[trial].scatter(all_pos.T[0], all_pos.T[1], alpha=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d3514-8055-48fc-8cb1-bb93acf6c95b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp = 31\n",
    "\n",
    "fig, ax = pplt.subplots(ncols=3)\n",
    "ax.format(xlim=[0, 300], ylim=[0, 300])\n",
    "\n",
    "for trial in range(3):\n",
    "    eps = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t{trial}', 'rb'))\n",
    "    saved_actions = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t{trial}_acts', 'rb'))\n",
    "\n",
    "    pos = eps['pos']\n",
    "    dones = eps['dones']\n",
    "    all_pos = np.vstack(split_by_ep(pos, dones)[exp])\n",
    "    # plt.scatter(eps['pos'][start:end].T[0], eps['pos'][start:end].T[1])\n",
    "    ax[trial].scatter(all_pos.T[0], all_pos.T[1], alpha=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef8805-2d2a-4d82-8e78-544b6b3d1bef",
   "metadata": {},
   "source": [
    "**Combine trial 1 for fuller coverage on north trajectories and trial 3 for general improved directness of trajectories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fea32c-a553-4dfa-9775-b4bbb54ae43b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Generate Conserved Trajectories\n",
    "\n",
    "trials_from_t1 = [29, 31]\n",
    "eps1 = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t0', 'rb'))\n",
    "saved_actions1 = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t0_acts', 'rb'))\n",
    "eps2 = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t2', 'rb'))\n",
    "saved_actions2 = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t2_acts', 'rb'))\n",
    "\n",
    "#Check which episodes have enough directness to be considered for these conserved trajectories\n",
    "all_pos = split_by_ep(eps2['pos'], eps2['dones'])\n",
    "keep_idxs = []\n",
    "for i, pos in enumerate(all_pos):\n",
    "    direct = compute_directness(pos=pos)\n",
    "    if direct > 0.9:\n",
    "        keep_idxs.append(i)\n",
    "keep_idxs = np.array(keep_idxs)\n",
    "\n",
    "combined_eps = {}\n",
    "\n",
    "for key in eps1:\n",
    "    comb_d = []\n",
    "    d1 = split_by_ep(eps1[key], eps1['dones'])\n",
    "    d2 = split_by_ep(eps2[key], eps2['dones'])\n",
    "    \n",
    "    for t in range(len(d1)):\n",
    "        if t in trials_from_t1:\n",
    "            d = d1[t]\n",
    "        else:\n",
    "            d = d2[t]\n",
    "        \n",
    "        if t in keep_idxs:\n",
    "            comb_d.append(d)\n",
    "        \n",
    "    if len(comb_d[0].shape) == 2:\n",
    "        comb_d = np.vstack(comb_d)\n",
    "    else:\n",
    "        comb_d = np.concatenate(comb_d)\n",
    "    \n",
    "    combined_eps[key] = comb_d\n",
    "    \n",
    "\n",
    "pickle.dump(combined_eps, open(f'data/pdistal_rim_heatmap/width64_comb', 'wb'))\n",
    "\n",
    "combined_actions = []\n",
    "for t in range(len(saved_actions1)):\n",
    "    if t not in keep_idxs:\n",
    "        continue\n",
    "    if t in trials_from_t1:\n",
    "        combined_actions.append(saved_actions1[t])\n",
    "    else:\n",
    "        combined_actions.append(saved_actions2[t])\n",
    "        \n",
    "keep_start_points = np.array(start_points)[keep_idxs]\n",
    "keep_start_angles = np.array(start_angles)[keep_idxs]\n",
    "#Save the actions, start points and angles\n",
    "pickle.dump([combined_actions, keep_start_points, keep_start_angles], open(f'data/pdistal_rim_heatmap/width64_comb_acts', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df30450-253c-464b-b074-1618b90f5d80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = pplt.subplots(ncols=4)\n",
    "ax.format(xlim=[0, 300], ylim=[0, 300])\n",
    "\n",
    "for trial in range(3):\n",
    "    eps = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t{trial}', 'rb'))\n",
    "    saved_actions = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t{trial}_acts', 'rb'))\n",
    "\n",
    "    pos = eps['pos']\n",
    "    ax[trial].scatter(pos.T[0], pos.T[1], alpha=0.2)\n",
    "    \n",
    "p = combined_eps['pos']\n",
    "ax[3].scatter(p.T[0], p.T[1], alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d757a32-45f5-452e-b8da-b3f49c15bcb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Collect activations along fixed trajectories for each width network and trial of agent (width{width}_copied)\n",
    "\n",
    "To generate activation heatmaps, we record the activations of each model along the fixed trajectories from 1.1 (taken from effectively optimal 64 width agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ccd840-949d-4167-b211-21e53fd1b614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "widths = [2, 3, 4, 8, 16, 32, 64]\n",
    "num_trials = 10\n",
    "\n",
    "combined_actions, keep_start_points, keep_start_angles = pickle.load(open(f'data/pdistal_rim_heatmap/width64_comb_acts', 'rb'))\n",
    "\n",
    "\n",
    "for width in widths:\n",
    "    all_eps = []\n",
    "\n",
    "    for trial in tqdm(range(num_trials)):\n",
    "        model_name = f'nav_poster_netstructure/nav_pdistal_width{width}batch200'\n",
    "        model, obs_rms, kwargs = load_model_and_env(model_name, trial)\n",
    "\n",
    "        all_ep = []\n",
    "        for i in range(len(keep_start_points)):\n",
    "            copied_actions = lambda step: combined_actions[i][step]\n",
    "            kw = kwargs.copy()\n",
    "            kw['fixed_reset'] = [keep_start_points[i].copy(), keep_start_angles[i].copy()]\n",
    "            ep = forced_action_evaluate(model, obs_rms, seed=0, num_episodes=1, \n",
    "                                        env_kwargs=kw, data_callback=poster_data_callback,\n",
    "                                        with_activations=True, forced_actions=copied_actions)\n",
    "            all_ep.append(ep)\n",
    "        \n",
    "        eps = clean_eps(stack_all_ep(all_ep), prune_first=0, save_inview=False, save_seen=False)\n",
    "        all_eps.append(eps)\n",
    "        \n",
    "        pickle.dump(all_eps, open(f'data/pdistal_rim_heatmap/width{width}_copied', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9634d0f-445b-428b-95e7-f02851bc1699",
   "metadata": {},
   "source": [
    "## Convert collected activations to heatmaps through smoothing (rim_heatmaps)\n",
    "\n",
    "Use Gaussian smoothing to generate activation heatmaps from the forced trajectory activations for each agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c682e337-8f6f-4cbb-90c4-3a8687d6f574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "widths = [2, 3, 4, 8, 16, 32, 64]\n",
    "\n",
    "all_heatmaps = {}\n",
    "\n",
    "for width in tqdm(widths):\n",
    "    all_heatmaps[width] = []\n",
    "    all_eps = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_copied', 'rb'))\n",
    "    \n",
    "    for eps in all_eps:\n",
    "        heatmaps = []\n",
    "        \n",
    "        p = eps['pos']\n",
    "        a = eps['activ']\n",
    "        \n",
    "        for i in range(a.shape[1]):\n",
    "            heatmap = gaussian_smooth(p, a[:, i])\n",
    "            heatmaps.append(heatmap)\n",
    "            \n",
    "        all_heatmaps[width].append(heatmaps)\n",
    "        \n",
    "pickle.dump(all_heatmaps, open('data/pdistal_rim_heatmap/rim_heatmaps', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a43ff3a-6134-4e8e-bc3c-a4ffad835831",
   "metadata": {},
   "source": [
    "## Collect trajectories along rim with original policies to compute directness stats (width{width})\n",
    "\n",
    "Next, with the same rim initial conditions, allow the agents to perform their original policy to collect behavior statistics like directness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220abe6-de34-4c2f-b9c5-7abed193cae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "widths = [2, 3, 4, 8, 16, 32, 64]\n",
    "num_trials = 10\n",
    "\n",
    "#Starting around rim - First generate start points and angles\n",
    "WINDOW_SIZE = (300, 300)\n",
    "step_size = 10.\n",
    "xs = np.arange(0+step_size, WINDOW_SIZE[0], step_size)\n",
    "ys = np.arange(0+step_size, WINDOW_SIZE[1], step_size)\n",
    "# thetas = np.linspace(0, 2*np.pi, 12, endpoint=False)\n",
    "start_points = []\n",
    "start_angles = []\n",
    "for x in xs:\n",
    "    for y in [5., 295.]:\n",
    "        point = np.array([x, y])\n",
    "        angle = np.arctan2(150 - y, 150 - x)\n",
    "        start_points.append(point)\n",
    "        start_angles.append(angle)\n",
    "for y in ys:\n",
    "    for x in [5, 295]:\n",
    "        point = np.array([x, y])\n",
    "        angle = np.arctan2(150 - y, 150 - x)\n",
    "        start_points.append(point)\n",
    "        start_angles.append(angle)\n",
    "        \n",
    "start_points = np.vstack(start_points)\n",
    "\n",
    "def filter_all_ep_directness(all_ep, bound=0.9):\n",
    "    d = compute_directness(all_ep)\n",
    "    idxs = d > 0.9\n",
    "    d_ep = [ep for i, ep in enumerate(all_ep) if idxs[i]]\n",
    "    return d_ep\n",
    "\n",
    "\n",
    "for width in widths:\n",
    "    all_eps = []\n",
    "    all_eps_f = []\n",
    "    \n",
    "    for trial in tqdm(range(num_trials)):\n",
    "        model_name = f'nav_poster_netstructure/nav_pdistal_width{width}batch200'\n",
    "        model, obs_rms, kwargs = load_model_and_env(model_name, trial)\n",
    "\n",
    "        all_ep = []\n",
    "        for i in range(len(start_points)):\n",
    "            kw = kwargs.copy()\n",
    "            kw['fixed_reset'] = [start_points[i].copy(), start_angles[i].copy()]\n",
    "            ep = forced_action_evaluate(model, obs_rms, seed=0, num_episodes=1, \n",
    "                                        env_kwargs=kw, data_callback=poster_data_callback,\n",
    "                                        with_activations=True)\n",
    "            all_ep.append(ep)\n",
    "        \n",
    "        all_ep_f = filter_all_ep_directness(all_ep)\n",
    "        eps_f = clean_eps(stack_all_ep(all_ep_f), prune_first=0, save_inview=False, save_seen=False)\n",
    "        eps = clean_eps(stack_all_ep(all_ep), prune_first=0, save_inview=False, save_seen=False)\n",
    "        \n",
    "        all_eps.append(eps)\n",
    "        all_eps_f.append(eps_f)\n",
    "        \n",
    "        pickle.dump(all_eps, open(f'data/pdistal_rim_heatmap/width{width}', 'wb'))\n",
    "        pickle.dump(all_eps_f, open(f'data/pdistal_rim_heatmap/width{width}_filt', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8cc437-18d5-44c1-ac86-ea7247c32c1e",
   "metadata": {},
   "source": [
    "## Generate KMeans model (kmeans_heatmap_clusterer) and summarize clustering results and behaviors\n",
    "\n",
    "Generate KMeans clusterer model and summarize the data from width{width} files as well as clustered. The second block here needs to be run in order to produce figures in section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3642a01c-f91a-4c79-a18b-f47d1b0d34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import entropy\n",
    "\n",
    "heatmaps, heatmap_idx_to_model, heatmap_model_to_idxs = load_heatmaps()\n",
    "num_clusters = 9\n",
    "kmeans = KMeans(n_clusters=num_clusters, n_init=100, random_state=0)\n",
    "preds = kmeans.fit_predict(heatmaps)\n",
    "\n",
    "cluster_idxs = []\n",
    "for i in range(9):\n",
    "    cluster_idxs.append(preds == i)\n",
    "\n",
    "cluster_freqs = [cluster.sum() for cluster in cluster_idxs]\n",
    "print(entropy(cluster_freqs))\n",
    "\n",
    "fig, ax = pplt.subplots(refwidth=2)\n",
    "ax.bar(np.arange(9), [cluster.sum() for cluster in cluster_idxs])\n",
    "\n",
    "pickle.dump(kmeans, open('data/pdistal_rim_heatmap/kmeans_heatmap_clusterer', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16fb62f-a9ae-4e2f-a6e5-d522d3e15be4",
   "metadata": {},
   "source": [
    "### RUN FOR SECTION 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d133072-7dae-4799-9d8b-5a02ec971a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Older code that puts results into list for plots prior to 4.4. Probably will want to update\n",
    "those plotting code to use next block dictionary entries\n",
    "'''\n",
    "\n",
    "heatmaps, heatmap_idx_to_model, heatmap_model_to_idxs = load_heatmaps()\n",
    "kmeans = pickle.load(open('data/pdistal_rim_heatmap/kmeans_heatmap_clusterer', 'rb'))\n",
    "\n",
    "num_clusters = 9\n",
    "widths = [2, 3, 4, 8, 16, 32, 64]\n",
    "trials = 10\n",
    "\n",
    "def convert_labels_to_ratios(clabels):\n",
    "    #Convert a list of cluster labels into ratios\n",
    "    cluster_ratios = np.zeros(num_clusters)\n",
    "    for i in range(num_clusters):\n",
    "        cluster_ratios[i] = np.sum(clabels == i)\n",
    "    cluster_ratios = cluster_ratios / len(clabels)\n",
    "    return cluster_ratios\n",
    "\n",
    "\n",
    "# Summarize clustering and behavior statistics\n",
    "all_heatmaps = pickle.load(open('data/pdistal_rim_heatmap/rim_heatmaps', 'rb'))\n",
    "results = {}\n",
    "for width in widths:\n",
    "    results[width] = []\n",
    "    all_eps = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}', 'rb'))\n",
    "    for trial in range(trials):\n",
    "        eps = all_eps[trial]\n",
    "        directness = compute_directness(pos=eps['pos'])\n",
    "        \n",
    "        ep_dones = split_by_ep(eps['dones'], eps['dones'])\n",
    "        ep_lens = np.array([ep.shape[0] for ep in ep_dones])\n",
    "        success_rate = 1 - np.sum(ep_lens == 202) / len(ep_lens)\n",
    "        average_ep_len = np.mean(ep_lens)\n",
    "        average_succ_ep_len = np.mean(ep_lens[ep_lens < 202])\n",
    "        \n",
    "        acts = eps['actions']\n",
    "        act_ratios = np.array([np.sum(acts == i) for i in range(4)]) / len(acts)\n",
    "        \n",
    "        hms = np.vstack([hm.reshape(1, -1) for hm in all_heatmaps[width][trial]])\n",
    "        labels = kmeans.predict(hms)\n",
    "        ratios = convert_labels_to_ratios(labels)\n",
    "        \n",
    "        \n",
    "        results[width].append([labels, ratios, directness, success_rate, average_ep_len, average_succ_ep_len, act_ratios])\n",
    "        \n",
    "\n",
    "'''\n",
    "results is a dict indexed by width. Values are lists of model results\n",
    "Each list item is [labels, ratios, directness, success_rate, av_ep_len, av_succ_ep_len]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c5cf56-0ff8-41d5-8c7d-9004753e31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Newer code with dictionary entries\n",
    "'''\n",
    "\n",
    "heatmaps, heatmap_idx_to_model, heatmap_model_to_idxs = load_heatmaps()\n",
    "kmeans = pickle.load(open('data/pdistal_rim_heatmap/kmeans_heatmap_clusterer', 'rb'))\n",
    "\n",
    "num_clusters = 9\n",
    "widths = [2, 3, 4, 8, 16, 32, 64]\n",
    "trials = 10\n",
    "\n",
    "# Summarize clustering and behavior statistics\n",
    "all_heatmaps = pickle.load(open('data/pdistal_rim_heatmap/rim_heatmaps', 'rb'))\n",
    "results = {}\n",
    "for width in widths:\n",
    "    results[width] = []\n",
    "    all_eps = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}', 'rb'))\n",
    "    for trial in range(trials):\n",
    "        eps = all_eps[trial]\n",
    "        directness = compute_directness(pos=eps['pos'])\n",
    "        \n",
    "        ep_dones = split_by_ep(eps['dones'], eps['dones'])\n",
    "        ep_lens = np.array([ep.shape[0] for ep in ep_dones])\n",
    "        success_rate = 1 - np.sum(ep_lens == 202) / len(ep_lens)\n",
    "        average_ep_len = np.mean(ep_lens)\n",
    "        average_succ_ep_len = np.mean(ep_lens[ep_lens < 202])\n",
    "        \n",
    "        acts = eps['actions']\n",
    "        act_ratios = np.array([np.sum(acts == i) for i in range(4)]) / len(acts)\n",
    "        \n",
    "        hms = np.vstack([hm.reshape(1, -1) for hm in all_heatmaps[width][trial]])\n",
    "        labels = kmeans.predict(hms)\n",
    "        _, ratios = count_labels(labels, remove_zeros=False)\n",
    "        _, nonzero = count_labels(labels, remove_zeros=True)\n",
    "        hprime = np.sum(-nonzero * np.log(nonzero))        \n",
    "\n",
    "        \n",
    "        \n",
    "        results[width].append({\n",
    "            'cluster_labels': labels, \n",
    "            'cluster_ratios': ratios, \n",
    "            'directness': directness, \n",
    "            'success_rate': success_rate, \n",
    "            'avg_ep_len': average_ep_len, \n",
    "            'avg_succ_ep_len': average_succ_ep_len, \n",
    "            'act_ratios': act_ratios,\n",
    "            'shannon': hprime\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef15d1-9866-45eb-beea-3fa3fa11a8cc",
   "metadata": {},
   "source": [
    "# Heatmap Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502bc4a7-8313-4158-8bc6-dd09543916c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Heatmaps for all forced rim trajectories (rim_heatmaps)\n",
    "\n",
    "Convert the saved activations and trajectories from 1.1.1 to smoothed heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d6fd8-30c5-418b-a609-131f64cef46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "    \n",
    "pca = PCA()\n",
    "pca.fit(heatmaps)\n",
    "\n",
    "n_components = 30\n",
    "plt.plot(pca.explained_variance_[:n_components] / pca.explained_variance_.sum())\n",
    "ev = pca.explained_variance_\n",
    "ev[:n_components].sum() / ev.sum()\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "reduced = pca.fit_transform(heatmaps)\n",
    "\n",
    "pca2 = PCA(n_components=2)\n",
    "reduced2 = pca.fit_transform(heatmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc8b3f-f3cc-4f10-b499-4d9224b9cd66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_[:30].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675e3f4-340a-459a-b80f-fa1671fedc69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(reduced2.T[0], reduced2.T[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13f551-b54c-4d90-983d-c45815dbe831",
   "metadata": {},
   "source": [
    "KMeans seems to be doing something quite reasonable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf44b87-4883-4f7d-aa65-cc7c6302d594",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86774f89-7bc1-4996-ba0e-3e57a7dfaeae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "\n",
    "# dbscan = DBSCAN(eps=5, min_samples=10)\n",
    "# preds = dbscan.fit_predict(reduced)\n",
    "# preds\n",
    "\n",
    "kmeans = KMeans()\n",
    "preds = kmeans.fit_predict(reduced)\n",
    "# preds\n",
    "\n",
    "cluster_idxs = []\n",
    "for i in range(8):\n",
    "    cluster_idxs.append(preds == i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e1a45-7cb6-4d80-ba9e-ec0a92223160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = 0\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7385b35-21a1-4640-924a-9f804ddc7928",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = 1\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=nrows)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3de46a-2bdc-4f85-9cea-fe35f5087c71",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = 2\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=nrows)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db22bd-998a-42f8-8c0a-3a7dc5cab03f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = 3\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=nrows)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a3baf-9c32-4e14-8927-a1f406f3da97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Agglomerative Clustering\n",
    "\n",
    "Agglomerative clustering allows us to try to see if we can do a dendrogram as a measure of sanity on how many clusters should be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973796b6-d105-4826-bc84-a1faebd93ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    \n",
    "    for i, merge in  enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "    \n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "    \n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "# model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "# model = model.fit(heatmaps)\n",
    "\n",
    "# plot_dendrogram(model, p=5, truncate_mode='level')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb2aaad-9743-435f-8cff-c875fbf30297",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_threshold = 31\n",
    "\n",
    "model = AgglomerativeClustering(distance_threshold=distance_threshold, n_clusters=None)\n",
    "preds = model.fit_predict(heatmaps)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.set_xticks([])\n",
    "ax.plot([0, 3700], [distance_threshold, distance_threshold], '--', c='red3', linewidth=1)\n",
    "plot_dendrogram(model, truncate_mode='level', ax=ax, \n",
    "                color_threshold=distance_threshold, no_labels=True)\n",
    "\n",
    "\n",
    "num_clusters = int(np.max(preds) + 1)\n",
    "cluster_idxs = []\n",
    "for i in range(num_clusters):\n",
    "    cluster_idxs.append(preds == i)\n",
    "    \n",
    "plt.savefig(save + '1_1_2_agglomerative_dendrogram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471ac8f-626f-48fd-85fd-f20c27a8cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 0\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))\n",
    "    \n",
    "fig.save(save + f'1_1_2_cluster{cluster}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb455b-9964-4896-9d11-b59ecbeecdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 1\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))\n",
    "    \n",
    "fig.save(save + f'1_1_2_cluster{cluster}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2f89a-11fd-4f97-9aef-e5be214411d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 2\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))\n",
    "    \n",
    "fig.save(save + f'1_1_2_cluster{cluster}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec4de2-cee1-4614-ba69-1db5667397d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 3\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))\n",
    "    \n",
    "fig.save(save + f'1_1_2_cluster{cluster}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b75d15-b5f1-4c14-8023-fd01917e3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 4\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))\n",
    "    \n",
    "fig.save(save + f'1_1_2_cluster{cluster}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a7aaff-6f9b-427f-b532-fc09f3e94f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 5\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))\n",
    "    \n",
    "fig.save(save + f'1_1_2_cluster{cluster}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfc297-00f2-460d-8e58-3f5df0f541e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 6\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))\n",
    "    \n",
    "fig.save(save + f'1_1_2_cluster{cluster}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a2a740-00eb-4d5a-ad7a-fe7d0a62b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 7\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))\n",
    "    \n",
    "fig.save(save + f'1_1_2_cluster{cluster}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1cac37-7be2-4103-87e8-bbe18afc80f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 8\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))\n",
    "    \n",
    "fig.save(save + f'1_1_2_cluster{cluster}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb155eb-e99d-4ecd-9575-ec81a27c66c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Checking whether those agents with circular trajectories have differing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32014957-a8ac-48f8-b2ce-b3c74988f229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "width = 4\n",
    "trial = 2\n",
    "\n",
    "ep = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t{trial}', 'rb'))\n",
    "\n",
    "\n",
    "heatmap_idxs = heatmap_model_to_idxs[width][trial]\n",
    "specific_heatmaps = heatmaps[heatmap_idxs[0]:heatmap_idxs[1]]\n",
    "specific_heatmaps = specific_heatmaps.reshape(-1, 30, 30)\n",
    "\n",
    "fig, ax = pplt.subplots([[0,1,1,0],[2,2,3,3],[4,4,5,5]])\n",
    "\n",
    "p = ep['pos']\n",
    "ax[0].scatter(p.T[0], p.T[1], alpha=0.2)\n",
    "for i in range(4):\n",
    "    ax[i+1].imshow(specific_heatmaps[i], extent=(5, 295, 5, 295))\n",
    "    \n",
    "fig.save(save + '1_1_3_example_4width_circling_trajectories.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c34f1-f420-4402-ade8-6ac65c1f46e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_labels_to_bar(labels, num_classes=9):\n",
    "    heights = []\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        heights.append(np.sum(labels == i))\n",
    "        \n",
    "    return np.array(heights)\n",
    "\n",
    "widths = [4, 8, 16, 32, 64]\n",
    "# widths = [4]\n",
    "trials = 3\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=len(widths), ncols=trials, share=False)\n",
    "taxs = ax.panel('t', space=0, share=False)\n",
    "\n",
    "colors = pplt.Cycle('default').by_key()['color']\n",
    "hex_to_rgb = lambda h: tuple(int(h.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))\n",
    "rgb_colors = np.array([hex_to_rgb(color) for color in colors])/255\n",
    "\n",
    "titles = []\n",
    "\n",
    "ax_idx = 0\n",
    "for width in widths:\n",
    "    for trial in range(trials):\n",
    "        ep = pickle.load(open(f'data/pdistal_rim_heatmap/width{width}_t{trial}', 'rb'))\n",
    "        p = ep['pos']\n",
    "        \n",
    "        heatmap_idxs = heatmap_model_to_idxs[width][trial]\n",
    "        # specific_heatmaps = heatmaps[heatmap_idxs[0]:heatmap_idxs[1]]\n",
    "        # specific_heatmaps = specific_heatmaps.reshape(-1, 30, 30)\n",
    "        \n",
    "        labels = preds[heatmap_idxs[0]:heatmap_idxs[1]]\n",
    "        \n",
    "        ax[ax_idx].scatter(p.T[0], p.T[1], alpha=0.2)\n",
    "        taxs[ax_idx].bar(convert_labels_to_bar(labels), colors=rgb_colors)\n",
    "        \n",
    "        titles.append(f'Width {width}, #{trial}')\n",
    "        ax_idx += 1\n",
    "                      \n",
    "ax.format(title=titles, leftlabels=[f'Width {width}' for width in widths],\n",
    "         suptitle='Original Trajectories Starting from Edge Initial Conditions and Clustering Classes of Nodes Undeer Forced Actions')\n",
    "taxs.format(xlocator=[], ylocator=[])\n",
    "fig.save(save + '1_1_3_trajectories_and_classes.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc885a-927c-4786-866b-7930e67d8833",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Checking some strange forced node behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98978e92-d7aa-4f09-93ea-59f56c63b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'nav_poster_netstructure/nav_pdistal_width64batch200'\n",
    "model, obs_rms, kwkargs = load_model_and_env(model_name, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15fa966-7ceb-4a54-bbe6-34eca3b11a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "[param.shape for param in model.base.gru.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62153286-d850-4a8f-be7e-dc9d26b1b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.base.gru.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c3d3de-56af-4505-aeb0-bfd58c7700be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(params[0][:64, 1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e3e93-343d-4bd8-9124-885a7bc57be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(nrows=8, ncols=8, share=True)\n",
    "for i in range(64):\n",
    "    # ax[i].hist(params[0][i, :].detach().numpy())\n",
    "    ax[i].hist(params[1][i+64, :].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988059e8-1a1c-4162-a96c-9244ab44fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots()\n",
    "for i in range(64):\n",
    "    ax.scatter([i]*64, params[1][:64, i].detach().numpy(), c='blue', alpha=0.2)\n",
    "    ax.scatter([i], params[1][1, i].detach().numpy(), c='black', marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2be95-b84e-4781-ae1b-8a968fb79bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "150d37d1-94f6-4d60-8533-c2d8777a986c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# KMeans Clustering Tests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e7be5-371a-40e9-b15f-fbacae3c2b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Generating KMeans Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ea196-d02e-4ec7-8f6c-5a2606681e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "centers = kmeans.cluster_centers_\n",
    "dists = [np.sqrt(np.sum((centers - reduced[i])**2, axis=0).min()) for i in range(reduced.shape[0])]\n",
    "\n",
    "heatmap_idxs = np.argsort(dists)[:16]\n",
    "ph = heatmaps[heatmap_idxs].reshape(-1, 30, 30)\n",
    "\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))\n",
    "\n",
    "labels = preds[heatmap_idxs]\n",
    "ax.format(title=[f'Cluster {label}' for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b03063e-2f06-435d-b6e3-08ef49a51a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AgglomerativeClustering(distance_threshold=distance_threshold, n_clusters=None)\n",
    "preds = model.fit_predict(heatmaps)\n",
    "num_clusters = int(np.max(preds) + 1)\n",
    "cluster_idxs = []\n",
    "for i in range(num_clusters):\n",
    "    cluster_idxs.append(preds == i)\n",
    "cluster_freqs = [cluster.sum() for cluster in cluster_idxs]\n",
    "\n",
    "entropy(cluster_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a45f3f-895c-4e75-a25d-bc5806b370e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = 0\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75213ea-705b-48f2-9a50-81a77351630e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = 1\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53a8c8-b7de-4e2f-a620-a587f61e38a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = 2\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc16d15-79ad-4826-b52b-a820cde34e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = 3\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef01b6-aee1-42cf-9b1a-5eeee48d92e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = 4\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff0aae-f7e1-42ea-bc80-9d0276f0cc0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = 5\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162ec61-cbd2-4091-85a9-d7fda7b6c547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = 6\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938dcde2-08cb-480c-8d34-b6c664d701f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = 7\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66121d51-71c5-4032-b597-bb85df32f832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = 8\n",
    "ph = heatmaps[cluster_idxs[cluster], :].reshape(-1, 30, 30)\n",
    "num_heatmaps = ph.shape[0]\n",
    "nrows = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "ncols = int(np.ceil(np.sqrt(num_heatmaps)))\n",
    "if ncols*(nrows-1) >= ph.shape[0]:\n",
    "    nrows = nrows-1\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=nrows, ncols=ncols)\n",
    "for i in range(ph.shape[0]):\n",
    "    ax[i].imshow(ph[i], extent=(5, 295, 5, 295))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e68522-2cfb-4346-9188-82b5b7bb98d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Directness and richness experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7bbf1c-72a9-4ca1-9f53-369c6d58f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(nrows=3, ncols=3)\n",
    "\n",
    "ax.format(title=[f'Cluster {c}' for c in range(num_clusters)])\n",
    "\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    x = []\n",
    "    y = []\n",
    "    for width in widths:\n",
    "        \n",
    "        ress = results[width]\n",
    "        for res in ress:\n",
    "            x.append(res[1][i])\n",
    "            y.append(res[2])\n",
    "    \n",
    "    ax[i].scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7ea4b-1d07-4455-a79f-ba5f3d8e99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(nrows=3, ncols=3)\n",
    "\n",
    "ax.format(title=[f'Cluster {c}' for c in range(num_clusters)])\n",
    "\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    x = []\n",
    "    y = []\n",
    "    for width in widths:\n",
    "        \n",
    "        ress = results[width]\n",
    "        for res in ress:\n",
    "            x.append(res[1][i])\n",
    "            y.append(res[4])\n",
    "    \n",
    "    ax[i].scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6e450-dcb9-49f5-97bf-81368c043dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Richenss measure:\n",
    "# * Ignore Cluster 1\n",
    "# * Ignore largest cluster showing\n",
    "# * For remaining clusters, take the minimal proportion\n",
    "#     Richness is num_clusters * min_proportion\n",
    "\n",
    "fig, ax = pplt.subplots()\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for width in widths:\n",
    "\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        ratios = res[1].copy()\n",
    "        ratios[1] = 0\n",
    "        # ratios[ratios.argmax()] = 0\n",
    "        nonzero = ratios[ratios != 0]\n",
    "        \n",
    "        if len(nonzero) > 0:\n",
    "            min_proportion = nonzero.min()\n",
    "            richness = min_proportion * len(nonzero)**2\n",
    "        else:\n",
    "            richness = 0\n",
    "        \n",
    "        x.append(richness)\n",
    "        y.append(res[2])\n",
    "        # np.arg\n",
    "        \n",
    "ax.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa36d2-5bd2-4356-aedc-27b0f51155c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Richness\n",
    "\n",
    "fig, ax = pplt.subplots(ncols=2)\n",
    "ax.format(title=['Richness', 'Menhinicks Index'])\n",
    "x = []\n",
    "x2 = []\n",
    "y = []\n",
    "\n",
    "\n",
    "\n",
    "for width in widths:\n",
    "\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        ratios = res[1].copy()\n",
    "        ratios[1] = 0\n",
    "        # ratios[ratios.argmax()] = 0\n",
    "        nonzero = ratios[ratios != 0]\n",
    "        \n",
    "        richness = len(nonzero)\n",
    "        # rel_richness = richness / np.sqrt(width)\n",
    "        rel_richness = richness / np.log(width)\n",
    "        x.append(richness)\n",
    "        x2.append(rel_richness)\n",
    "        y.append(res[2])\n",
    "        # np.arg\n",
    "        \n",
    "ax[0].scatter(x, y)\n",
    "ax[1].scatter(x2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31974b-1754-4782-8ee5-57ac85bbde48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diversity measures\n",
    "\n",
    "fig, ax = pplt.subplots(ncols=2, nrows=2, share=False)\n",
    "ax.format(title=['Shanon-Wiener Index', 'Simpsons Dominance', 'Shanon Number', 'Simpsons Number'])\n",
    "x = []\n",
    "x2 = []\n",
    "x3 = []\n",
    "x4 = []\n",
    "y = []\n",
    "\n",
    "for width in widths:\n",
    "\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        counts, ratios = count_labels(res[0], remove_zeros=True)\n",
    "\n",
    "        hprime = np.sum(-ratios * np.log(ratios))        \n",
    "        lambd = np.sum(counts * (counts-1)) / (width*(width-1))\n",
    "\n",
    "        \n",
    "        x.append(hprime)\n",
    "        x2.append(1 - lambd)\n",
    "        x3.append(np.exp(hprime))\n",
    "        x4.append(1/(1-lambd))\n",
    "        y.append(res[2])\n",
    "        \n",
    "        # np.arg\n",
    "        \n",
    "ax[0].scatter(x, y)\n",
    "ax[1].scatter(x2, y)\n",
    "ax[2].scatter(x3, y)\n",
    "ax[3].scatter(x4, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77166024-8f27-4d80-83e6-d719e7c3dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.load(open(f'data/pdistal_rim_heatmap/width16_t1', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1223455-3414-4a0b-9f31-73026579b18f",
   "metadata": {},
   "source": [
    "## Shannon-Wiener"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226a875-12aa-4963-beb5-11c77b45ae5d",
   "metadata": {},
   "source": [
    "### Success Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69050d1-2bca-4239-ab9e-786a0f88cfc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Diversity measures\n",
    "\n",
    "def count_labels(clabels, ignore_cluster=None, remove_zeros=False):\n",
    "    #Convert a list of cluster labels into ratios\n",
    "    cluster_counts = np.zeros(num_clusters)\n",
    "    for i in range(num_clusters):\n",
    "        cluster_counts[i] = np.sum(clabels == i)\n",
    "        \n",
    "    if ignore_cluster is not None:\n",
    "        if type(ignore_cluster) == list:\n",
    "            for c in ignore_cluster:\n",
    "                cluster_counts[c] = 0\n",
    "        elif type(ignore_cluster) == int:\n",
    "            cluster_counts[ignore_cluster] = 0\n",
    "    \n",
    "    cluster_ratios = cluster_counts / np.sum(cluster_counts)\n",
    "    \n",
    "    if remove_zeros:\n",
    "        cluster_ratios = cluster_ratios[cluster_ratios != 0]\n",
    "        cluster_counts = cluster_counts[cluster_counts != 0]\n",
    "    return cluster_counts, cluster_ratios\n",
    "\n",
    "title = ['Directness', 'Success Rate', 'Average Ep Length', \n",
    "                 'Average Successful Ep Length']\n",
    "fig, ax = pplt.subplots(ncols=2, nrows=2, share=False)\n",
    "ax.format(title=title)\n",
    "x = []\n",
    "ys = {t: [] for t in title}\n",
    "\n",
    "for width in widths:\n",
    "\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        counts, ratios = count_labels(res[0], remove_zeros=True)\n",
    "        hprime = np.sum(-ratios * np.log(ratios))        \n",
    "        \n",
    "        \n",
    "        a\n",
    "        x.append(hprime)\n",
    "        ys['Directness'].append(res[2]) \n",
    "        ys['Success Rate'].append(res[3]) \n",
    "        ys['Average Ep Length'].append(res[4])\n",
    "        ys['Average Successful Ep Length'].append(res[5])\n",
    "        \n",
    "x = np.array(x)\n",
    "for i, t in enumerate(title):\n",
    "    ys[t] = np.array(ys[t])\n",
    "    ax[i].scatter(x, ys[t])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90877b-6fb0-4bd0-a3c7-f9321a2ecf03",
   "metadata": {},
   "source": [
    "**Penalizing Cluster 1 richness**\n",
    "\n",
    "To penalize, we simply add p_1*log(p_1) to the richness score (basically removing the positive contribution of p_1 to richness)\n",
    "\n",
    "This seems like it helps smooth an outlier in the Directness graph, but adds an outlier to the Average Successfull Ep Length graph. So it seems unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bcc1f3-173f-4c4a-a7f1-3d8fc32954b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Diversity measures\n",
    "\n",
    "def count_labels(clabels, ignore_cluster=None, remove_zeros=False):\n",
    "    #Convert a list of cluster labels into ratios\n",
    "    cluster_counts = np.zeros(num_clusters)\n",
    "    for i in range(num_clusters):\n",
    "        cluster_counts[i] = np.sum(clabels == i)\n",
    "        \n",
    "    if ignore_cluster is not None:\n",
    "        if type(ignore_cluster) == list:\n",
    "            for c in ignore_cluster:\n",
    "                cluster_counts[c] = 0\n",
    "        elif type(ignore_cluster) == int:\n",
    "            cluster_counts[ignore_cluster] = 0\n",
    "    \n",
    "    cluster_ratios = cluster_counts / np.sum(cluster_counts)\n",
    "    \n",
    "    if remove_zeros:\n",
    "        cluster_ratios = cluster_ratios[cluster_ratios != 0]\n",
    "        cluster_counts = cluster_counts[cluster_counts != 0]\n",
    "    return cluster_counts, cluster_ratios\n",
    "\n",
    "title = ['Directness', 'Success Rate', 'Average Ep Length', \n",
    "                 'Average Successful Ep Length']\n",
    "fig, ax = pplt.subplots(ncols=2, nrows=2, share=False)\n",
    "ax.format(title=title)\n",
    "x2 = []\n",
    "ys2 = {t: [] for t in title}\n",
    "\n",
    "for width in widths:\n",
    "\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        counts, ratios = count_labels(res[0], remove_zeros=False)\n",
    "        nonzero = ratios[ratios != 0]\n",
    "        hprime = np.sum(-nonzero * np.log(nonzero))\n",
    "        #further penalize the 1 cluster?\n",
    "        if ratios[1] != 0:\n",
    "            hprime += ratios[1] * np.log(ratios[1])\n",
    "        \n",
    "        \n",
    "        x2.append(hprime)\n",
    "        ys2['Directness'].append(res[2]) \n",
    "        ys2['Success Rate'].append(res[3]) \n",
    "        ys2['Average Ep Length'].append(res[4])\n",
    "        ys2['Average Successful Ep Length'].append(res[5])\n",
    "        \n",
    "x2 = np.array(x2)\n",
    "for i, t in enumerate(title): \n",
    "    ys2[t] = np.array(ys2[t])\n",
    "    ax[i].scatter(x2, ys2[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b721d018-fefe-419b-8de0-6f17bbe9458f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Compare with simple num nodes\n",
    "\n",
    "def count_labels(clabels, ignore_cluster=None, remove_zeros=False):\n",
    "    #Convert a list of cluster labels into ratios\n",
    "    cluster_counts = np.zeros(num_clusters)\n",
    "    for i in range(num_clusters):\n",
    "        cluster_counts[i] = np.sum(clabels == i)\n",
    "        \n",
    "    if ignore_cluster is not None:\n",
    "        if type(ignore_cluster) == list:\n",
    "            for c in ignore_cluster:\n",
    "                cluster_counts[c] = 0\n",
    "        elif type(ignore_cluster) == int:\n",
    "            cluster_counts[ignore_cluster] = 0\n",
    "    \n",
    "    cluster_ratios = cluster_counts / np.sum(cluster_counts)\n",
    "    \n",
    "    if remove_zeros:\n",
    "        cluster_ratios = cluster_ratios[cluster_ratios != 0]\n",
    "        cluster_counts = cluster_counts[cluster_counts != 0]\n",
    "    return cluster_counts, cluster_ratios\n",
    "\n",
    "title = ['Richness', 'Directness', 'Success Rate', 'Average Ep Length', \n",
    "                 'Average Successful Ep Length']\n",
    "array = [[0, 1, 1, 0],\n",
    "         [2, 2, 3, 3],\n",
    "         [4, 4, 5, 5]]\n",
    "fig, ax = pplt.subplots(array, share=False)\n",
    "ax.format(title=title)\n",
    "x2 = []\n",
    "ys2 = {t: [] for t in title}\n",
    "\n",
    "for width in widths:\n",
    "\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        counts, ratios = count_labels(res['cluster_labels'], remove_zeros=False)\n",
    "        nonzero = ratios[ratios != 0]\n",
    "        hprime = np.sum(-nonzero * np.log(nonzero))\n",
    "        #further penalize the 1 cluster?\n",
    "        if ratios[1] != 0:\n",
    "            hprime += ratios[1] * np.log(ratios[1])\n",
    "        \n",
    "        \n",
    "        # x2.append(hprime)\n",
    "        x2.append(width)\n",
    "        ys2['Richness'].append(res['shannon'])\n",
    "        ys2['Directness'].append(res['directness']) \n",
    "        ys2['Success Rate'].append(res['success_rate']) \n",
    "        ys2['Average Ep Length'].append(res['avg_ep_len'])\n",
    "        ys2['Average Successful Ep Length'].append(res['avg_succ_ep_len'])\n",
    "        \n",
    "x2 = np.array(x2)\n",
    "ax[0].scatter(x2, ys2['Richness'])\n",
    "plot_titles = ['Directness', 'Success Rate', 'Average Ep Length', \n",
    "                 'Average Successful Ep Length']\n",
    "for i, t in enumerate(plot_titles): \n",
    "    ys2[t] = np.array(ys2[t])\n",
    "    ax[i+1].scatter(x2, ys2[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe23ff-7bb4-4115-8ab4-01c3da6ff762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Compare with simple num nodes\n",
    "\n",
    "def count_labels(clabels, ignore_cluster=None, remove_zeros=False):\n",
    "    #Convert a list of cluster labels into ratios\n",
    "    cluster_counts = np.zeros(num_clusters)\n",
    "    for i in range(num_clusters):\n",
    "        cluster_counts[i] = np.sum(clabels == i)\n",
    "        \n",
    "    if ignore_cluster is not None:\n",
    "        if type(ignore_cluster) == list:\n",
    "            for c in ignore_cluster:\n",
    "                cluster_counts[c] = 0\n",
    "        elif type(ignore_cluster) == int:\n",
    "            cluster_counts[ignore_cluster] = 0\n",
    "    \n",
    "    cluster_ratios = cluster_counts / np.sum(cluster_counts)\n",
    "    \n",
    "    if remove_zeros:\n",
    "        cluster_ratios = cluster_ratios[cluster_ratios != 0]\n",
    "        cluster_counts = cluster_counts[cluster_counts != 0]\n",
    "    return cluster_counts, cluster_ratios\n",
    "\n",
    "title = ['Directness', 'Success Rate', 'Average Ep Length', \n",
    "                 'Average Successful Ep Length']\n",
    "fig, ax = pplt.subplots(nrows=2, ncols=2, share=False)\n",
    "ax.format(title=title)\n",
    "x2 = []\n",
    "ys2 = {t: [] for t in title}\n",
    "\n",
    "color_plots = []\n",
    "\n",
    "for n, width in enumerate(widths):\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        x2.append(res['shannon'])\n",
    "        ys2['Directness'].append(res['directness']) \n",
    "        ys2['Success Rate'].append(res['success_rate']) \n",
    "        ys2['Average Ep Length'].append(res['avg_ep_len'])\n",
    "        ys2['Average Successful Ep Length'].append(res['avg_succ_ep_len'])\n",
    "        color_plots.append(n)\n",
    "        \n",
    "x2 = np.array(x2)\n",
    "color_plots = np.array(color_plots)\n",
    "\n",
    "for j, width in enumerate(widths):\n",
    "    ps = color_plots == j\n",
    "    for i, t in enumerate(plot_titles): \n",
    "        ys2[t] = np.array(ys2[t])\n",
    "        ax[i].scatter(x2[ps], ys2[t][ps])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34027b9-dd42-4cb2-916a-f45ee92bd8fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Linear Regressions\n",
    "\n",
    "Here we fit a couple linear regressions to see whether it is important to penalize a model from having cluster 1. From the below results, it seems again that adding this penalization (described above) does not improve r2 scores. Ultimately it looks like leaving the diversity measure to simply be Shannon-Wiener instead of coming up with an arbitrary penalization is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f97a4b-8c49-44f3-a551-de35ebb2e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "lr = LinearRegression()\n",
    "lr.fit(x.reshape(-1, 1), ys['Directness'].reshape(-1, 1))\n",
    "ypred = lr.predict(x.reshape(-1, 1))\n",
    "print(lr.coef_, lr.intercept_, r2_score(ys['Directness'].reshape(-1, 1), ypred))\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(x2.reshape(-1, 1), ys2['Directness'].reshape(-1, 1))\n",
    "ypred = lr.predict(x2.reshape(-1, 1))\n",
    "print(lr.coef_, lr.intercept_, r2_score(ys2['Directness'].reshape(-1, 1), ypred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22312f10-8d2d-4ee1-9481-e38de7620392",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(x.reshape(-1, 1), ys['Average Successful Ep Length'].reshape(-1, 1))\n",
    "ypred = lr.predict(x.reshape(-1, 1))\n",
    "print(lr.coef_, lr.intercept_, r2_score(ys['Average Successful Ep Length'].reshape(-1, 1), ypred))\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(x2.reshape(-1, 1), ys2['Average Successful Ep Length'].reshape(-1, 1))\n",
    "ypred = lr.predict(x2.reshape(-1, 1))\n",
    "print(lr.coef_, lr.intercept_, r2_score(ys2['Average Successful Ep Length'].reshape(-1, 1), ypred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0deab7b-6e99-4e56-a204-9d65eb2ff975",
   "metadata": {},
   "source": [
    "## Action Ratios\n",
    "\n",
    "It seems like agents may have a directional preference (in particular, a cw/ccw bias). We would like to see what factors influence the bias, and if the bias affects ultimate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961423fd-610b-4cad-823e-3a47f6a5e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0-2 action ratios\n",
    "fig, ax = pplt.subplots()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for width in widths:\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        act_ratios = res[6]\n",
    "        x.append(act_ratios[0])\n",
    "        y.append(act_ratios[2])\n",
    "\n",
    "ax.scatter(x, y)\n",
    "ax.format(xlabel='Ratio of Left Turns', ylabel='Ratio of Right Turns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a046979-910b-4f67-bcc5-ce8dcdbbae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0-2 action ratios\n",
    "fig, ax = pplt.subplots(ncols=3)\n",
    "\n",
    "title = ['Left', 'Forward', 'Right']\n",
    "ax.format(title=title)\n",
    "\n",
    "xs = {t: [] for t in title}\n",
    "y = []\n",
    "\n",
    "\n",
    "for width in widths:\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        act_ratios = res[6]\n",
    "        xs['Left'].append(act_ratios[0])\n",
    "        xs['Forward'].append(act_ratios[1])\n",
    "        xs['Right'].append(act_ratios[2])\n",
    "        y.append(res[2])\n",
    "\n",
    "for i, t in enumerate(title):\n",
    "    ax[i].scatter(xs[t], y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de9155-b518-4378-b73e-d65ea572fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(nrows=3, ncols=3)\n",
    "\n",
    "xs = [[], [], [], [], [], [], [], [], []]\n",
    "ys = [[], [], []]\n",
    "\n",
    "for width in widths:\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        act_ratios = res[6]\n",
    "        c_ratios = res[1]\n",
    "        for i in range(3):\n",
    "            ys[i].append(act_ratios[i])\n",
    "        for i in range(9):\n",
    "            xs[i].append(c_ratios[i])\n",
    "\n",
    "for i in range(9):\n",
    "    for j in range(3):\n",
    "        ax[i].scatter(xs[i], ys[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7ef3b-9576-41c3-8014-95d75cb950e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(nrows=3, ncols=3)\n",
    "\n",
    "xs = [[], [], [], [], [], [], [], [], []]\n",
    "ys = []\n",
    "\n",
    "for width in widths:\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        act_ratios = res[6]\n",
    "        c_ratios = res[1]\n",
    "        lr_pref = act_ratios[0] / (act_ratios[0] + act_ratios[1])\n",
    "        #ratio of left:right\n",
    "        ys.append(lr_pref)\n",
    "        \n",
    "        for i in range(9):\n",
    "            xs[i].append(c_ratios[i])\n",
    "\n",
    "for i in range(9):\n",
    "    # for j in range(3):\n",
    "    ax[i].scatter(xs[i], ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b428c-c899-4930-ac48-6e0401825ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc8008e-6cc3-4922-aa40-ace370986a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.array(xs).T\n",
    "y = np.array(ys).reshape(-1, 1)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "pred = lr.predict(X)\n",
    "\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd5be1-0bb0-4a61-b16d-a154f31a3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, ReLU, Sequential\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for width in widths:\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        act_ratios = res['act_ratios']\n",
    "        lr_ratio = act_ratios[0] / (act_ratios[0] + act_ratios[2])\n",
    "        xs.append(res['cluster_ratios'])\n",
    "        ys.append(lr_ratio)\n",
    "        \n",
    "\n",
    "lm = Sequential(Linear(9, 4),  ReLU(), Linear(4, 1))\n",
    "X = np.array(xs)\n",
    "y = torch.tensor(np.array(ys).reshape(-1, 1), dtype=torch.float32)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "opt = Adam(lm.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeae8a9-0e66-4f52-ac8e-476315465a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "losses = []\n",
    "for i in range(1000):\n",
    "    pred = lm(X_train)\n",
    "    loss = mse_loss(pred, y_train)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    losses.append(loss.item())\n",
    "    opt.step()\n",
    "\n",
    "pred_train = lm(X_train)\n",
    "pred_test = lm(X_test)\n",
    "\n",
    "fig, ax = pplt.subplots(ncols=2)\n",
    "ax[0].scatter(pred_train.T[0].detach(), y_train.T[0])\n",
    "ax[1].scatter(pred_test.T[0].detach(), y_test.T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f176eaf-a445-4f7e-9eb1-7a7ad25ab6a8",
   "metadata": {},
   "source": [
    "### Directness/Richness vs LR Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f02c18-0109-408e-82b3-6f4bb8f8e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "xs2 = []\n",
    "ys = []\n",
    "\n",
    "fig, ax = pplt.subplots(ncols=2, sharex=False)\n",
    "\n",
    "for width in widths:\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        act_ratios = res['act_ratios']\n",
    "        lr_ratio = act_ratios[0] / (act_ratios[0] + act_ratios[2])\n",
    "        xs.append(res['directness'])\n",
    "        xs2.append(res['shannon'])\n",
    "        ys.append(2*lr_ratio - 1)\n",
    "        \n",
    "ax[0].scatter(xs, ys, alpha=0.5)\n",
    "ax[1].scatter(xs2,  ys, alpha=0.5)\n",
    "ax[0].format(xlabel='directness', ylabel='LR Ratio')\n",
    "ax[1].format(xlabel='richness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5c89f-1dfe-4994-abe5-4d49614f8aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "fig, ax = pplt.subplots(nrows=3, ncols=3)\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for width in widths:\n",
    "    ress = results[width]\n",
    "    for res in ress:\n",
    "        act_ratios = res['act_ratios']\n",
    "        lr_ratio = act_ratios[0] / (act_ratios[0] + act_ratios[2])\n",
    "        xs.append(res['cluster_ratios'])\n",
    "        ys.append(lr_ratio)\n",
    "        \n",
    "for i in range(9):\n",
    "    ax[i].scatter(np.array(xs).T[i], (np.array(ys)-0.5)*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c95ed-d31f-4551-9cec-a7cccf84d002",
   "metadata": {},
   "source": [
    "# Development of Clusters over training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a85a2-223c-4bff-b5e6-45c2b32da4aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Generation\n",
    "\n",
    "Ended up moving this into fil run_checkpoint_analysis.py to be able to run this on remote computers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c968b8-b74d-44c8-b172-5b25fee5c9cf",
   "metadata": {},
   "source": [
    "### Heatmap from conserved trajectory activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6092bab-2c8f-4f14-a33e-ca74c46f8204",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 64\n",
    "trial = 0\n",
    "data_folder = 'data/pdistal_rim_heatmap/'\n",
    "data_path = data_folder + f'width{width}_checkpoint'\n",
    "checkpoint_data = pickle.load(open(data_path, 'rb'))\n",
    "\n",
    "heatmap_path = data_folder + f'width{width}_checkpoint_hms'\n",
    "if not Path(heatmap_path).exists():\n",
    "    heatmap_data = {}\n",
    "else:\n",
    "    heatmap_data = pickle.load(open(heatmap_path, 'rb'))\n",
    "\n",
    "if trial not in heatmap_data: \n",
    "    heatmap_data[trial] = {}\n",
    "\n",
    "for chkp_val in tqdm(checkpoint_data['copied'][trial]):\n",
    "    eps = checkpoint_data['copied'][trial][chkp_val]\n",
    "    heatmaps = []\n",
    "\n",
    "    p = eps['pos']\n",
    "    a = eps['activ']\n",
    "\n",
    "    for i in range(a.shape[1]):\n",
    "        heatmap = gaussian_smooth(p, a[:, i])\n",
    "        heatmaps.append(heatmap)\n",
    "    \n",
    "    heatmap_data[trial][chkp_val] = heatmaps\n",
    "        \n",
    "pickle.dump(heatmap_data, open(heatmap_path, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b331b5c-8aea-4e7a-8e05-1a09a42f66eb",
   "metadata": {},
   "source": [
    "### Summary of policy pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede1a27-379e-4d38-9c8c-b70b1125dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Older code that puts results into list for plots prior to 4.4. Probably will want to update\n",
    "those plotting code to use next block dictionary entries\n",
    "'''\n",
    "\n",
    "kmeans = pickle.load(open('data/pdistal_rim_heatmap/kmeans_heatmap_clusterer', 'rb'))\n",
    "\n",
    "width = 64\n",
    "trial = 0\n",
    "\n",
    "\n",
    "data_folder = 'data/pdistal_rim_heatmap/'\n",
    "data_path = data_folder + f'width{width}_checkpoint'\n",
    "checkpoint_data = pickle.load(open(data_path, 'rb'))\n",
    "\n",
    "heatmap_path = data_folder + f'width{width}_checkpoint_hms'\n",
    "summary_path = data_folder + f'width{width}_checkpoint_summ'\n",
    "\n",
    "heatmap_data = pickle.load(open(heatmap_path, 'rb'))\n",
    "if not Path(summary_path).exists():\n",
    "    summary_data = {}\n",
    "else:\n",
    "    summary_data = pickle.load(open(summary_path, 'rb'))\n",
    "\n",
    "if trial not in summary_data: \n",
    "    summary_data[trial] = {}\n",
    "\n",
    "for chkp_val in tqdm(checkpoint_data['policy'][trial]):    \n",
    "    eps = checkpoint_data['policy'][trial][chkp_val]\n",
    "    directness = compute_directness(pos=eps['pos'])\n",
    "\n",
    "    ep_dones = split_by_ep(eps['dones'], eps['dones'])\n",
    "    ep_lens = np.array([ep.shape[0] for ep in ep_dones])\n",
    "    success_rate = 1 - np.sum(ep_lens == 202) / len(ep_lens)\n",
    "    average_ep_len = np.mean(ep_lens)\n",
    "    average_succ_ep_len = np.mean(ep_lens[ep_lens < 202])\n",
    "\n",
    "    acts = eps['actions']\n",
    "    act_ratios = np.array([np.sum(acts == i) for i in range(4)]) / len(acts)\n",
    "\n",
    "    hms = np.vstack([hm.reshape(1, -1) for hm in heatmap_data[trial][chkp_val]])\n",
    "    labels = kmeans.predict(hms)\n",
    "    _, ratios = count_labels(labels, remove_zeros=False)\n",
    "    _, nonzero = count_labels(labels, remove_zeros=True)\n",
    "    hprime = np.sum(-nonzero * np.log(nonzero))        \n",
    "\n",
    "\n",
    "\n",
    "    summary_data[trial][chkp_val] = {\n",
    "        'cluster_labels': labels, \n",
    "        'cluster_ratios': ratios, \n",
    "        'directness': directness, \n",
    "        'success_rate': success_rate, \n",
    "        'avg_ep_len': average_ep_len, \n",
    "        'avg_succ_ep_len': average_succ_ep_len, \n",
    "        'act_ratios': act_ratios,\n",
    "        'shannon': hprime\n",
    "    }\n",
    "    \n",
    "pickle.dump(summary_data, open(summary_path, 'wb'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afdda1b-88c4-4061-a13d-56ba5207805a",
   "metadata": {},
   "source": [
    "## Analysis of development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f0ba3-e41a-4800-a4a8-4637abea68ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compression Test\n",
    "\n",
    "Looks like we can effectively compress heatmaps to 16-bit (half) precision, saving 75% of space for transfer. Visually the heatmaps look identical, and clustering produces identical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad02651-be38-4e4c-a2be-7ad6c87cad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_folder = 'data/pdistal_rim_heatmap/'\n",
    "data_path = data_folder + f'width{width}_checkpoint'\n",
    "heatmap_path = data_folder + f'width{width}_checkpoint_hms'\n",
    "summary_path = data_folder + f'width{width}_checkpoint_summ'\n",
    "\n",
    "heatmaps = pickle.load(open(heatmap_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff9e49-5f64-4d65-bf58-7089cd672aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_half = {}\n",
    "\n",
    "for t in heatmaps:\n",
    "    heatmap_half[t] = {}\n",
    "    for chk in heatmaps[t]:\n",
    "        heatmap_half[t][chk] = []\n",
    "        for hm in heatmaps[t][chk]:\n",
    "            heatmap_half[t][chk].append(hm.astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f3384-9350-4102-89c2-84589e0cc9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(heatmap_half, open(data_folder+f'width{width}_checkpoint_hms_half', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e184a-5ce5-46d4-879b-0b23d3c1194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_half[t][chk].append(hm.astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9435b-e5ec-4b4a-8c84-0d681c58c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "chks = [0, 100, 200, 300, 400, 500]\n",
    "fig, ax = pplt.subplots(nrows=2, ncols=len(chks))\n",
    "for i, chk in enumerate(chks):\n",
    "    ax[0, i].imshow(heatmaps[0][chk][0])\n",
    "    ax[1, i].imshow(heatmap_half[0][chk][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26515aed-38cf-4f8f-8c76-0a38a7a74e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chk in heatmaps[0]:\n",
    "    l1 = pred_kmeans(heatmaps[0][chk], kmeans)\n",
    "    l2 = pred_kmeans(heatmap_half[0][chk], kmeans)\n",
    "    if not (l1 == l2).all():\n",
    "        print(chk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b57c272-f65e-497d-b544-7411d9e97e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979177e3-f2ab-4354-b6dd-ad8a1debf0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for chkp in summary_data[0]:\n",
    "    xs.append(chkp)\n",
    "    ys.append(summary_data[0][chkp]['shannon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ead94-ab38-41fb-aea5-2e1957943318",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(nrows=2, ncols=5)\n",
    "for i in range(10):\n",
    "    chkp = list(heatmap_data[0].keys())[i]\n",
    "    \n",
    "    hm = heatmap_data[0][chkp][0]\n",
    "    ax[i].imshow(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f833f8b-1f51-4c07-bd1a-8077f04251c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 64\n",
    "trial = 0\n",
    "data_folder = 'data/pdistal_rim_heatmap/'\n",
    "data_path = data_folder + f'width{width}_checkpoint'\n",
    "checkpoint_data = pickle.load(open(data_path, 'rb'))\n",
    "\n",
    "heatmap_path = data_folder + f'width{width}_checkpoint_hms'\n",
    "if not Path(heatmap_path).exists():\n",
    "    heatmap_data = {}\n",
    "else:\n",
    "    heatmap_data = pickle.load(open(heatmap_path, 'rb'))\n",
    "\n",
    "if trial not in heatmap_data: \n",
    "    heatmap_data[trial] = {}\n",
    "\n",
    "    \n",
    "hms = []\n",
    "for i, chkp_val in tqdm(enumerate(checkpoint_data['copied'][trial])):\n",
    "    if i == 10:\n",
    "        break\n",
    "    eps = checkpoint_data['copied'][trial][chkp_val]\n",
    "    heatmaps = []\n",
    "\n",
    "    p = eps['pos']\n",
    "    a = eps['activ']\n",
    "\n",
    "    for i in range(1):\n",
    "        heatmap = gaussian_smooth(p, a[:, i])\n",
    "        heatmaps.append(heatmap)\n",
    "        hms.append(heatmap)\n",
    "    \n",
    "    heatmap_data[trial][chkp_val] = heatmaps\n",
    "        \n",
    "# pickle.dump(heatmap_data, open(heatmap_path, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6639e9-5ed7-4032-acae-bf226d3b3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(nrows=2, ncols=5)\n",
    "for i in range(10):\n",
    "    # chkp = list(heatmap_data[0].keys())[i]\n",
    "    \n",
    "    # hm = heatmap_data[0][chkp][0]\n",
    "    hm = hms[i]\n",
    "    ax[i].imshow(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441ef6a-3298-4d82-bcf8-86d3c3df1814",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Older code that puts results into list for plots prior to 4.4. Probably will want to update\n",
    "those plotting code to use next block dictionary entries\n",
    "'''\n",
    "\n",
    "kmeans = pickle.load(open('data/pdistal_rim_heatmap/kmeans_heatmap_clusterer', 'rb'))\n",
    "\n",
    "width = 64\n",
    "trial = 0\n",
    "\n",
    "\n",
    "data_folder = 'data/pdistal_rim_heatmap/'\n",
    "data_path = data_folder + f'width{width}_checkpoint'\n",
    "checkpoint_data = pickle.load(open(data_path, 'rb'))\n",
    "\n",
    "heatmap_path = data_folder + f'width{width}_checkpoint_hms'\n",
    "summary_path = data_folder + f'width{width}_checkpoint_summ'\n",
    "\n",
    "heatmap_data = pickle.load(open(heatmap_path, 'rb'))\n",
    "if not Path(summary_path).exists():\n",
    "    summary_data = {}\n",
    "else:\n",
    "    summary_data = pickle.load(open(summary_path, 'rb'))\n",
    "\n",
    "if trial not in summary_data: \n",
    "    summary_data[trial] = {}\n",
    "\n",
    "for chkp_val in tqdm(checkpoint_data['policy'][trial]):    \n",
    "    eps = checkpoint_data['policy'][trial][chkp_val]\n",
    "    directness = compute_directness(pos=eps['pos'])\n",
    "\n",
    "    ep_dones = split_by_ep(eps['dones'], eps['dones'])\n",
    "    ep_lens = np.array([ep.shape[0] for ep in ep_dones])\n",
    "    success_rate = 1 - np.sum(ep_lens == 202) / len(ep_lens)\n",
    "    average_ep_len = np.mean(ep_lens)\n",
    "    average_succ_ep_len = np.mean(ep_lens[ep_lens < 202])\n",
    "\n",
    "    acts = eps['actions']\n",
    "    act_ratios = np.array([np.sum(acts == i) for i in range(4)]) / len(acts)\n",
    "\n",
    "    hms = np.vstack([hm.reshape(1, -1) for hm in all_heatmaps[width][trial]])\n",
    "    labels = kmeans.predict(hms)\n",
    "    _, ratios = count_labels(labels, remove_zeros=False)\n",
    "    _, nonzero = count_labels(labels, remove_zeros=True)\n",
    "    hprime = np.sum(-nonzero * np.log(nonzero))        \n",
    "\n",
    "\n",
    "\n",
    "    summary_data[trial][chkp_val] = {\n",
    "        'cluster_labels': labels, \n",
    "        'cluster_ratios': ratios, \n",
    "        'directness': directness, \n",
    "        'success_rate': success_rate, \n",
    "        'avg_ep_len': average_ep_len, \n",
    "        'avg_succ_ep_len': average_succ_ep_len, \n",
    "        'act_ratios': act_ratios,\n",
    "        'shannon': hprime\n",
    "    }\n",
    "    \n",
    "pickle.dump(summary_data, open(summary_path, 'wb'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1866a7b-5593-4971-8f5b-5dbd78ec4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chk in summary_data[0]:\n",
    "    print(summary_data[0][chk]['directness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda9cec-11d3-461c-90a7-adb5dab0675e",
   "metadata": {},
   "source": [
    "## Evolution of Cluster Ratios over Training\n",
    "\n",
    "**IMPORTANT NOTE: Because of the way files were saved and named, checkpoints were not loaded in order and simply iterating over keys in dictionaries will lead to anachronistic results. To iterate, sort checkpoint names and iterate over sorted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f6251b-c3d0-4bb1-9551-d8a9c428c5ff",
   "metadata": {},
   "source": [
    "### Preliminary Examination\n",
    "\n",
    "On preliminary viewing, it looks like directness and average ep lengths improve regardless of changes to representative richness (for 16 width networks). However on closer inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a84b5-6269-46d9-b157-458eb3e2107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "trial = 0\n",
    "\n",
    "data_folder = 'data/pdistal_rim_heatmap/'\n",
    "data_path = data_folder + f'width{width}_checkpoint'\n",
    "heatmap_path = data_folder + f'width{width}_checkpoint_hms_half'\n",
    "summary_path = data_folder + f'width{width}_checkpoint_summ'\n",
    "\n",
    "summ = pickle.load(open(summary_path, 'rb'))\n",
    "hms = pickle.load(open(heatmap_path, 'rb'))\n",
    "\n",
    "chks = list(summ[0].keys())\n",
    "chks = np.sort(chks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9fb616-4730-4ac8-8172-6866482dc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(nrows=3, ncols=3)\n",
    "for j in range(9):\n",
    "    all_ratios = np.vstack([summ[j][chk]['cluster_ratios'] for chk in chks])\n",
    "    for i in range(num_clusters):\n",
    "        ax[j].plot(all_ratios[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88918b6d-135f-4336-a4a2-df8688d95cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dccdea8-6976-4288-b2dc-a471831c6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(shannons.shape[0]):\n",
    "    plt.plot(pd.Series(shannons[i]).ewm(alpha=0.1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5870ed8-6ffb-4c05-9bb7-812f5adfc544",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(nrows=3, ncols=3)\n",
    "\n",
    "shannons = []\n",
    "directnesses = []\n",
    "avg_ep_lens = []\n",
    "for j in summ:\n",
    "    shannon = np.vstack([summ[j][chk]['shannon'] for chk in chks]).squeeze()\n",
    "    directness = np.vstack([summ[j][chk]['directness'] for chk in chks]).squeeze()\n",
    "    avg_ep_len = np.vstack([summ[j][chk]['avg_ep_len'] for chk in chks]).squeeze()\n",
    "    shannons.append(shannon)\n",
    "    directnesses.append(directness)\n",
    "    avg_ep_lens.append(avg_ep_len)\n",
    "\n",
    "\n",
    "for i in range(9):\n",
    "    richness = pd.Series(shannons[i]).ewm(alpha=0.3).mean()\n",
    "    directness = pd.Series(directnesses[i]).ewm(alpha=0.3).mean()\n",
    "    avg_ep_len = pd.Series(avg_ep_lens[i]).ewm(alpha=0.3).mean() / 200\n",
    "    # richness = shannons[i]\n",
    "    # directness = directnesses[i]\n",
    "    ax[i].plot(richness, label='richness')\n",
    "    ax[i].plot(directness, label='directness')\n",
    "    ax[i].plot(avg_ep_len, label='avg_ep_len')\n",
    "    \n",
    "ax[0].legend(loc='ur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4cd34-c834-4c2b-ab53-d04cf434ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(nrows=3, ncols=3)\n",
    "\n",
    "width = 8\n",
    "\n",
    "data_folder = 'data/pdistal_rim_heatmap/'\n",
    "data_path = data_folder + f'width{width}_checkpoint'\n",
    "heatmap_path = data_folder + f'width{width}_checkpoint_hms_half'\n",
    "summary_path = data_folder + f'width{width}_checkpoint_summ'\n",
    "\n",
    "summ = pickle.load(open(summary_path, 'rb'))\n",
    "hms = pickle.load(open(heatmap_path, 'rb'))\n",
    "\n",
    "chks = list(summ[0].keys())\n",
    "chks = np.sort(chks)\n",
    "\n",
    "shannons = []\n",
    "directnesses = []\n",
    "avg_ep_lens = []\n",
    "for j in summ:\n",
    "    shannon = np.vstack([summ[j][chk]['shannon'] for chk in chks]).squeeze()\n",
    "    directness = np.vstack([summ[j][chk]['directness'] for chk in chks]).squeeze()\n",
    "    avg_ep_len = np.vstack([summ[j][chk]['avg_ep_len'] for chk in chks]).squeeze()\n",
    "    shannons.append(shannon)\n",
    "    directnesses.append(directness)\n",
    "    avg_ep_lens.append(avg_ep_len)\n",
    "\n",
    "\n",
    "for i in range(9):\n",
    "    richness = pd.Series(shannons[i]).ewm(alpha=0.3).mean()\n",
    "    directness = pd.Series(directnesses[i]).ewm(alpha=0.3).mean()\n",
    "    avg_ep_len = pd.Series(avg_ep_lens[i]).ewm(alpha=0.3).mean() / 200\n",
    "    # richness = shannons[i]\n",
    "    # directness = directnesses[i]\n",
    "    ax[i].plot(richness, label='richness')\n",
    "    ax[i].plot(directness, label='directness')\n",
    "    ax[i].plot(avg_ep_len, label='avg_ep_len')\n",
    "    \n",
    "ax[0].legend(loc='ur')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f561b3c-9533-4e41-a912-4f994dd1e317",
   "metadata": {},
   "source": [
    "### Differentials\n",
    "\n",
    "Two interesting notes\n",
    "* It seems like a lot of richness is almost decided at random initiation?! Specifically it seems like a model that starts with a lot of cluster 1 will continue to have it towards the end\n",
    "    * We definitely need to spend some time exploring how individual nodes evolve over training besides ensemble metrics. Is it that individual nodes stay the same or that it just happens that as an ensemble the clustering ratios are preserved?\n",
    "* Despite this dependence on initial clustering and that all models qualitatively show the same behavior of learning, there do seem to be quantitative differences in how learning progresses\n",
    "    * There are noticeable areas of rapid improvement vs. steadier improvement, which seem correlated with increases and decreases in richness\n",
    "    * **In this section, we are trying to see whether the checkpoint to checkpoint differentials in richness can be statistically correlated with differentials in directness or performance as measured by ep length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb00e5c-e3b6-443d-b843-f05d5f739647",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "trial = 0\n",
    "\n",
    "data_folder = 'data/pdistal_rim_heatmap/'\n",
    "data_path = data_folder + f'width{width}_checkpoint'\n",
    "heatmap_path = data_folder + f'width{width}_checkpoint_hms_half'\n",
    "summary_path = data_folder + f'width{width}_checkpoint_summ'\n",
    "\n",
    "summ = pickle.load(open(summary_path, 'rb'))\n",
    "hms = pickle.load(open(heatmap_path, 'rb'))\n",
    "\n",
    "chks = list(summ[0].keys())\n",
    "chks = np.sort(chks)\n",
    "\n",
    "shannons = []\n",
    "directnesses = []\n",
    "avg_ep_lens = []\n",
    "for j in summ:\n",
    "    shannon = np.vstack([summ[j][chk]['shannon'] for chk in chks]).squeeze()\n",
    "    directness = np.vstack([summ[j][chk]['directness'] for chk in chks]).squeeze()\n",
    "    avg_ep_len = np.vstack([summ[j][chk]['avg_ep_len'] for chk in chks]).squeeze()\n",
    "    shannons.append(shannon)\n",
    "    directnesses.append(directness)\n",
    "    avg_ep_lens.append(avg_ep_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d12c7-ad04-4c9d-9a5a-977d22eb652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shannon_diffs = []\n",
    "directness_diffs = []\n",
    "avg_ep_len_diffs = []\n",
    "checkpoints = []\n",
    "\n",
    "for t in range(len(shannons)):\n",
    "    shannon_diffs.append(np.diff(shannons[t]))\n",
    "    directness_diffs.append(np.diff(directnesses[t]))\n",
    "    avg_ep_len_diffs.append(np.diff(avg_ep_lens[t]))\n",
    "    checkpoints.append(chks[:-1])\n",
    "    \n",
    "shannon_diffs = np.concatenate(shannon_diffs)\n",
    "directness_diffs = np.concatenate(directness_diffs)\n",
    "avg_ep_len_diffs = np.concatenate(avg_ep_len_diffs)\n",
    "checkpoints = np.concatenate(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58192dd3-708a-44ba-8403-c1b8ed40dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(shannon_diffs, directness_diffs, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05e2da-692c-4322-a23d-81a2ebc160de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(checkpoints, directness_diffs, alpha=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc508168-bb30-4abf-a623-adfc46b6805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([checkpoints, shannon_diffs]).T\n",
    "y = directness_diffs.reshape(-1, 1)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39ee307-f9cd-43bf-954c-33bbaaf8ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195d8b8-b898-4d52-a76c-2021ec0b0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938c7ed-9559-471b-98e4-07ffd6860a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(y.T, y_pred.T)[0, 1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c60ee-ca6e-494a-8394-d4e5e08fef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X.T, y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e7429e-c617-41da-9243-7d8d3fd8d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shannon_diffs = []\n",
    "directness_diffs = []\n",
    "avg_ep_len_diffs = []\n",
    "checkpoints = []\n",
    "\n",
    "for t in range(len(shannons)):\n",
    "    shannon_diffs.append(np.diff(shannons[t]))\n",
    "    directness_diffs.append(np.diff(directnesses[t]))\n",
    "    avg_ep_len_diffs.append(np.diff(avg_ep_lens[t]))\n",
    "    checkpoints.append(chks[:-1])\n",
    "    \n",
    "shannon_diffs = np.vstack(shannon_diffs)\n",
    "directness_diffs = np.vstack(directness_diffs)\n",
    "avg_ep_len_diffs = np.vstack(avg_ep_len_diffs)\n",
    "checkpoints = np.vstack(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d031a-f40a-4daa-8716-b353a46ea7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "directness_diffs[0][2:-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b01c94d-73d9-40f2-adf6-e59c9aeb4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "directness_diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d809d20-b1c2-44d1-895d-ad80925ff41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([shannon_diffs[:, i:i-4].reshape(-1) for i in range(4)])\n",
    "y = directness_diffs[:, 2:-2].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f086d-28ff-4920-bc11-e2478166e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a02cea-7aee-44a1-a755-dfd0a69891d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436df70d-3d95-4c1f-9ffe-f001c9156ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de1f55b-25fb-4d26-9ee1-d047d6218e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pplt.subplots(nrows=2, ncols=2)\n",
    "for i in range(4):\n",
    "    ax[i].scatter(X[i], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24c9b1-6bcc-46e4-930d-6b85d27ecd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
